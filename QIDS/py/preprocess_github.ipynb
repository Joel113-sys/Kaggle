{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# from lightgbm import LGBMRegressor\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "n_fold = 10\n",
    "group_gap = 31\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/first_round_train_market_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joellau/Desktop/Kaggle/QIDS/py/preprocess_github.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/Kaggle/QIDS/py/preprocess_github.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.max_columns\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m350\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/Kaggle/QIDS/py/preprocess_github.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#read data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joellau/Desktop/Kaggle/QIDS/py/preprocess_github.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m df_train_market \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(TRAIN_MARKET_PATH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/Kaggle/QIDS/py/preprocess_github.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m df_train_return \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(TRAIN_RETURN_PATH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/Kaggle/QIDS/py/preprocess_github.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m df_train_fundamental \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(TRAIN_FUNADMENTAL_PATH)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/first_round_train_market_data.csv'"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = \"../../data/\"\n",
    "TRAIN_MARKET_PATH = f'{ROOT_PATH}first_round_train_market_data.csv'\n",
    "# TRAIN_MARKET_PATH = f'{ROOT_PATH}train.csv'\n",
    "TRAIN_FUNADMENTAL_PATH = f'{ROOT_PATH}first_round_train_fundamental_data.csv'\n",
    "TRAIN_RETURN_PATH = f'{ROOT_PATH}first_round_train_return_data.csv'\n",
    "\n",
    "TEST_ROOT_PATH = \"../qids_package/\"\n",
    "TEST_MARKET_PATH = f'{TEST_ROOT_PATH}first_round_test_market_data.csv'\n",
    "# TEST_MARKET_PATH = f'{ROOT_PATH}test.csv'\n",
    "TEST_FUNADMENTAL_PATH = f'{TEST_ROOT_PATH}first_round_test_fundamental_data.csv'\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 350)\n",
    "\n",
    "#read data\n",
    "df_train_market = pd.read_csv(TRAIN_MARKET_PATH)\n",
    "df_train_return = pd.read_csv(TRAIN_RETURN_PATH)\n",
    "df_train_fundamental = pd.read_csv(TRAIN_FUNADMENTAL_PATH)\n",
    "\n",
    "df_test_market = pd.read_csv(TEST_MARKET_PATH)\n",
    "df_test_fundamental = pd.read_csv(TEST_FUNADMENTAL_PATH)\n",
    "\n",
    "#merge train dataset and test dataset\n",
    "def split_time(x):\n",
    "    df1 = x['date_time'].str.split('d', expand=True)\n",
    "    df1.columns=['code','s']\n",
    "    code = df1['code']\n",
    "    df1 = df1['s'].str.split('p', expand=True)\n",
    "    df1.columns=['day','time_step']\n",
    "    df2 = x['date_time'].str.rsplit('p', expand=True)\n",
    "    df2.columns=['day_s','s']\n",
    "    df1['day_s'] = df2['day_s']\n",
    "    df1['code'] = code\n",
    "    x = pd.concat([x,df1],axis=1)\n",
    "    return x\n",
    "\n",
    "df_train_market = split_time(df_train_market)\n",
    "df = pd.merge(df_train_fundamental,df_train_market, left_on='date_time',right_on='day_s')  \n",
    "df = pd.merge(df,df_train_return, left_on='day_s',right_on='date_time')  \n",
    "\n",
    "df_test_market = split_time(df_test_market)\n",
    "test = pd.merge(df_test_fundamental,df_test_market, left_on='date_time',right_on='day_s')  \n",
    "\n",
    "#drop duplicates\n",
    "df = df.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)\n",
    "test = test.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)\n",
    "\n",
    "def growth(data, features, group):\n",
    "    \n",
    "    \"\"\"\n",
    "    create growth rate column based on selected features\n",
    "    \"\"\"\n",
    "    \n",
    "    grouped = data.groupby(group)\n",
    "    \n",
    "    for feature in features:\n",
    "        data[f'{feature}_growth'] = grouped[feature].pct_change()\n",
    "        \n",
    "    return data\n",
    "\n",
    "def lag_feature_with_group(data, features, n, group):\n",
    "\n",
    "    \"\"\"\n",
    "    create a lagged column in data from feature with n lagging periods\n",
    "    \"\"\"\n",
    "    \n",
    "    grouped = data.groupby(group)\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        for feature in features:\n",
    "            data[f'{feature}_{i}'] = grouped[feature].shift(i)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def sma(data, features, n, group):\n",
    "    \n",
    "    \"\"\"\n",
    "    create sma(n) column in data from feature\n",
    "    \"\"\"\n",
    "    \n",
    "    grouped = data.groupby(group)\n",
    "    \n",
    "    for i in n:\n",
    "        for feature in features:\n",
    "            data[f'{feature}_sma{i}'] = grouped.rolling(i)[feature].mean().reset_index(drop=True)\n",
    "        \n",
    "    return data\n",
    "\n",
    "features = ['pe_ttm', 'pe', 'pb', 'ps', 'pcf']\n",
    "sma_periods = [10,25,50]\n",
    "df_lag = lag_feature_with_group(df, features, 2, 'code')\n",
    "df_sma = sma(df_lag, features, sma_periods, 'code')\n",
    "df_growth = growth(df_sma, features, 'code')\n",
    "# fig, ax = plt.subplots(figsize=(7,15))\n",
    "# sns.heatmap(df_lag.corr(numeric_only=True)[['return']].sort_values(by='return', ascending=False),annot=True);\n",
    "\n",
    "test = lag_feature_with_group(test, features, 2, 'code')\n",
    "test = sma(test, features, sma_periods, 'code')\n",
    "test = growth(test, features, 'code')\n",
    "\n",
    "# df = df.dropna()\n",
    "# test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 257248\n",
    "stock_num = 54\n",
    "train_day_num_total = 1000\n",
    "train_day_num = 1000 - 2\n",
    "test_day_num = 700\n",
    "timeslot_num = 50\n",
    "\n",
    "calc_log = lambda df: np.log(np.where(df > 1e-8, df, 1e-8))\n",
    "calc_mean = lambda df: df.mean(axis=0)\n",
    "calc_max = lambda df: df.max(axis=0)\n",
    "calc_min = lambda df: df.min(axis=0)\n",
    "calc_std = lambda df: df.std()\n",
    "calc_var = lambda df: df.var()\n",
    "calc_add = lambda df1, df2: df1 + df2\n",
    "calc_diff = lambda df1, df2: df1 - df2\n",
    "calc_prod = lambda df1, df2: df1 * df2\n",
    "calc_div = lambda df1, df2: df1 / df2\n",
    "\n",
    "def preprocess(fun, mar, ret=None):\n",
    "    fun[\"stock_id\"] = fun[\"date_time\"].apply(lambda x: x.split(\"d\")[0][1:]).astype(\"int\")\n",
    "    fun[\"day\"] = fun[\"date_time\"].apply(lambda x: x.split(\"d\")[1][:]).astype(\"int\")\n",
    "    # fun[\"log_pb\"] = calc_log(fun[\"pb\"])\n",
    "    # fun[\"log_ps\"] = calc_log(fun[\"ps\"])\n",
    "    fun = fun.sort_values(by=[\"stock_id\", \"day\"])\n",
    "    na_fun = fun.loc[fun[\"day\"].isin([999, 1000])]\n",
    "    fun = fun.drop(na_fun.index, axis=0).reset_index(drop=True)\n",
    "    na_fun = na_fun.reset_index(drop=True)\n",
    "\n",
    "    mar[\"stock_id\"] = mar[\"date_time\"].apply(lambda x: x.split(\"d\")[0][1:]).astype(\"int\")\n",
    "    mar[\"day\"] = mar[\"date_time\"].apply(lambda x: x.split(\"d\")[1].split(\"p\")[0]).astype(\"int\")\n",
    "    mar[\"time\"] = mar[\"date_time\"].apply(lambda x: x.split(\"p\")[1]).astype(\"int\")\n",
    "    mar = mar.sort_values(by=[\"stock_id\", \"day\", \"time\"]).reset_index(drop=True)\n",
    "    na_mar = mar.loc[mar[\"day\"].isin([999, 1000])]\n",
    "    mar = mar.drop(na_mar.index, axis=0).reset_index(drop=True)\n",
    "    na_mar = na_mar.reset_index(drop=True)\n",
    "\n",
    "    combined = copy.deepcopy(fun)\n",
    "    if ret is not None:\n",
    "        ret[\"stock_id\"] = ret[\"date_time\"].apply(lambda x: x.split(\"d\")[0][1:]).astype(\"int\")\n",
    "        ret[\"day\"] = ret[\"date_time\"].apply(lambda x: x.split(\"d\")[1][:]).astype(\"int\")\n",
    "        # ret[\"log_pb\"] = calc_log(ret[\"pb\"])\n",
    "        # ret[\"log_ps\"] = calc_log(ret[\"ps\"])\n",
    "        ret = ret.sort_values(by=[\"stock_id\", \"day\"]).reset_index(drop=True)\n",
    "        combined[\"return\"] = ret[\"return\"]\n",
    "        day_num = train_day_num\n",
    "    else:\n",
    "        day_num = test_day_num\n",
    "\n",
    "    mar_summary = []\n",
    "    start = 0\n",
    "    for stock in range(stock_num):\n",
    "        end = start + day_num * timeslot_num\n",
    "        stock_info = mar.iloc[start:end, :]\n",
    "        day_start = 0\n",
    "        for day in range(day_num):\n",
    "            day_end = day_start + timeslot_num\n",
    "            stock_info_per_day = stock_info.iloc[day_start:day_end, :]\n",
    "            mar_summary.append([\n",
    "                calc_mean(stock_info_per_day[\"open\"]),\n",
    "                calc_mean(stock_info_per_day[\"close\"]),\n",
    "                calc_mean(stock_info_per_day[\"high\"]),\n",
    "                calc_mean(stock_info_per_day[\"low\"]),\n",
    "                calc_mean(stock_info_per_day[\"volume\"]),\n",
    "                calc_mean(stock_info_per_day[\"money\"]),\n",
    "                calc_max(stock_info_per_day[\"high\"]),\n",
    "                calc_max(stock_info_per_day[\"volume\"]),\n",
    "                calc_max(stock_info_per_day[\"money\"]),\n",
    "                calc_min(stock_info_per_day[\"low\"]),\n",
    "                calc_min(stock_info_per_day[\"volume\"]),\n",
    "                calc_min(stock_info_per_day[\"money\"]),\n",
    "                calc_std(stock_info_per_day[\"volume\"]),\n",
    "                calc_std(stock_info_per_day[\"money\"]),\n",
    "                calc_var(stock_info_per_day[\"volume\"]),\n",
    "                calc_var(stock_info_per_day[\"money\"]),\n",
    "                calc_max(calc_div(calc_diff(stock_info_per_day[\"close\"], stock_info_per_day[\"open\"]), stock_info_per_day[\"open\"])),\n",
    "                calc_max(calc_div(calc_diff(stock_info_per_day[\"high\"], stock_info_per_day[\"low\"]), stock_info_per_day[\"open\"])),\n",
    "            ])\n",
    "            day_start = day_end\n",
    "        start = end\n",
    "    cols = [\n",
    "        \"open_mean\",\n",
    "        \"close_mean\",\n",
    "        \"high_mean\",\n",
    "        \"low_mean\",\n",
    "        \"volume_mean\",\n",
    "        \"money_mean\",\n",
    "        \"high_max\",\n",
    "        \"volume_max\",\n",
    "        \"money_max\",\n",
    "        \"low_min\",\n",
    "        \"volume_min\",\n",
    "        \"money_min\",\n",
    "        \"volume_std\",\n",
    "        \"money_std\",\n",
    "        \"volume_var\",\n",
    "        \"money_var\",\n",
    "        \"price_diff\",\n",
    "        \"price_diff_max\",\n",
    "    ]\n",
    "    mar_summary = pd.DataFrame(mar_summary, columns=cols)\n",
    "    combined = pd.concat([combined, mar_summary], axis=1)\n",
    "\n",
    "    return [combined, fun, mar, na_fun, na_mar, ret] if ret is not None else [combined, fun, mar, na_fun, na_mar]\n",
    "\n",
    "train = df.drop(columns=[\"date_time_x\", \"date_time_y\", \"day_s\", \"code\"]).fillna(0)\n",
    "test = test.rename(columns={\"date_time_x\": \"date_time\"}).drop(columns=[\"date_time_y\", \"day_s\", \"code\"]).fillna(0)\n",
    "\n",
    "train_fun, train_mar, train_ret = train, df_train_market, df_train_return\n",
    "test_fun, test_mar = test, df_test_market\n",
    "\n",
    "train_combined, train_fun, train_mar, train_na_fun, train_na_mar, train_ret = preprocess(train_fun, train_mar, train_ret)\n",
    "test_combined, test_fun, test_mar, test_na_fun, test_na_mar = preprocess(test_fun, test_mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder(df):\n",
    "    df_cols = df.columns\n",
    "    if 'return' not in df_cols:\n",
    "        df_cols_prior = ['date_time', 'stock_id', 'day']\n",
    "    else:\n",
    "        df_cols_prior = ['date_time', 'stock_id', 'day', 'return']\n",
    "    for col in df_cols:\n",
    "        if col not in df_cols_prior:\n",
    "            df_cols_prior.append(col)\n",
    "    if 'return' in df_cols_prior:\n",
    "        df_cols_prior.remove('return')\n",
    "        df_cols_prior.append('return')\n",
    "    return df[df_cols_prior]\n",
    "\n",
    "train = reorder(train_combined)\n",
    "test = reorder(test_combined)\n",
    "\n",
    "train.to_csv(\"../../data/train_github.csv\", index=False)\n",
    "test.to_csv(\"../../data/test_github.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
