{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HKU QIDS 2023 Quantitative Investment Competition: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from qids_package.qids import *\n",
    "import warnings\n",
    "from submit import submit\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 257248\n",
    "stock_num = 54\n",
    "day_num_total = 1000\n",
    "day_num = 1000 - 2\n",
    "test_day_num = 700\n",
    "timeslot_num = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(train, valid, test=None):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    valid = scaler.transform(valid)\n",
    "    if test is not None:\n",
    "        test = scaler.transform(test)\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr(df1, df2):\n",
    "    new_df = pd.concat([df1,df2],axis=1)\n",
    "    return new_df.corr().iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train, valid, test, train_y, valid_y, return_pred=True, version=2, return_auc=False, plot_auc=False):\n",
    "    model.fit(train, train_y)\n",
    "    if version == 2:\n",
    "        model_train_y = model.predict(train)\n",
    "        model_valid_y = model.predict(valid)\n",
    "        pred = model.predict(test)\n",
    "        acr_train = model.score(train, train_y)\n",
    "        acr_valid = model.score(valid, valid_y)\n",
    "        print(calc_corr(train_y, model_train_y))\n",
    "        print(calc_corr(valid_y, model_valid_y))\n",
    "        if return_pred:\n",
    "            return pred\n",
    "\n",
    "def evaluate2(model, train, test, train_y, real_y, return_pred=True, version=2, return_auc=False, plot_auc=False):\n",
    "    model.fit(train, train_y)\n",
    "    if version == 2:\n",
    "        model_train_y = model.predict(train)\n",
    "        pred = np.array(model.predict(test)).reshape(-1,1)\n",
    "        pred_temp = pred[:37692]\n",
    "        real_y = np.array(real_y).reshape(-1,1)\n",
    "        train_y = pd.DataFrame(train_y)\n",
    "        model_train_y = pd.DataFrame(model_train_y)\n",
    "        combined0 = pd.concat([train_y,model_train_y],axis = 1)\n",
    "        # print(combined0.corr())\n",
    "        pred_temp = pd.DataFrame(pred_temp)\n",
    "        real_y = pd.DataFrame(real_y)\n",
    "        combined = pd.concat([pred_temp,real_y],axis = 1)\n",
    "        # print(combined.corr())\n",
    "        if return_pred:\n",
    "            return pd.DataFrame(pred),combined.corr().iloc[1,0]\n",
    "def evaluate3(model, train, test, train_y, real_y, return_pred=True, version=2, return_auc=False, plot_auc=False):\n",
    "    model.fit(train, train_y)\n",
    "    if version == 2:\n",
    "        model_train_y = model.predict(train)\n",
    "        pred = np.array(model.predict(test)).reshape(-1,1)\n",
    "        pred_temp = pred[:37692]\n",
    "        real_y = np.array(real_y).reshape(-1,1)\n",
    "        # print(real_y.shape)\n",
    "        # print(pred.shape)\n",
    "        # print(\"train_score\",calc_corr(train_y, model_train_y))\n",
    "        # print(\"prediction_score\",calc_corr(pred, real_y))\n",
    "        train_y = pd.DataFrame(train_y)\n",
    "        model_train_y = pd.DataFrame(model_train_y)\n",
    "        combined0 = pd.concat([train_y,model_train_y],axis = 1)\n",
    "        # print(combined0.corr())\n",
    "        pred_temp = pd.DataFrame(pred_temp)\n",
    "        real_y = pd.DataFrame(real_y)\n",
    "        combined = pd.concat([pred_temp,real_y],axis = 1)\n",
    "        # print(combined.corr())\n",
    "        if return_pred:\n",
    "            return pred, combined.corr().iloc[1,0]\n",
    "        \n",
    "#train 54 models respectively\n",
    "# def evaluate4(model, train_lst, test_lst, train_y_lst, real_y_lst):\n",
    "#     fit_lst = []\n",
    "#     pred_lst = []\n",
    "#     for i in range(54):\n",
    "#         model_sample = copy.deepcopy(model)\n",
    "#         model_sample.fit(train_lst[i], train_y_lst[i])\n",
    "#         fit_lst.append(model_sample.predict(train_lst[i])) \n",
    "#         pred_lst.append(np.array(model_sample.predict(test_lst[i])).reshape(-1,1))\n",
    "#         # # pred_temp_sample = pred_sample #[:37692] #暂时先不管最后两天\n",
    "#         # train_y_sample = pd.DataFrame(train_y_sample)\n",
    "#         # model_train_y_sample = pd.DataFrame(model_train_y_sample)\n",
    "#         # # combined0 = pd.concat([train_y_sample,model_train_y_sample],axis = 1)\n",
    "#         # pred_temp_sample = pd.DataFrame(pred_temp_sample)\n",
    "#         # real_y_sample = pd.DataFrame(real_y_sample)\n",
    "#         # combined = pd.concat([pred_temp_sample,real_y_sample],axis = 1)\n",
    "#         # sample_lst.append(pred_sample)\n",
    "#     # combined1 = pd.concat(sample_lst, axis=1)\n",
    "#     combined2 = pd.concat(pred_lst, axis=1)\n",
    "#     # print(combined1.corr())\n",
    "#     return combined2\n",
    "\n",
    "#多次\n",
    "def evaluate4(model, train, test, train_y, real_y, return_pred=True, version=2, return_auc=False, plot_auc=False):\n",
    "    model.fit(train, train_y)\n",
    "    if version == 2:\n",
    "        model_train_y = np.array(model.predict(train)).reshape(-1,1)\n",
    "        pred = np.array(model.predict(test)).reshape(-1,1)\n",
    "        pred_temp = pred[:698]\n",
    "        real_y = np.array(real_y).reshape(-1,1)\n",
    "        train_y = pd.DataFrame(train_y)\n",
    "        model_train_y = pd.DataFrame(model_train_y)\n",
    "        combined0 = pd.concat([train_y,model_train_y],axis = 1)\n",
    "        print('training correlation',combined0.corr().iloc[1,0])\n",
    "        # print(\"len of train_y\", len(train_y))\n",
    "        # print(\"len of model_train_y:\", len(model_train_y))\n",
    "        pred_temp = pd.DataFrame(pred_temp)\n",
    "        real_y = pd.DataFrame(real_y)\n",
    "        # print(\"lengths:\",len(pred_temp), len(real_y))\n",
    "        combined = pd.concat([pred_temp,real_y],axis = 1)\n",
    "        # print(combined.corr())\n",
    "        if return_pred:\n",
    "            return pd.DataFrame(pred),combined.corr().iloc[1,0], model_train_y,train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"../data/\"\n",
    "\n",
    "train_path = write_path + \"train.csv\"\n",
    "valid_path = write_path + \"valid.csv\"\n",
    "test_path = write_path + \"test.csv\"\n",
    "real_return_path = write_path + 'real_return.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "valid = pd.read_csv(valid_path)\n",
    "test = pd.read_csv(test_path)\n",
    "real_return = pd.read_csv(real_return_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>day</th>\n",
       "      <th>turnoverRatio</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>pe_ttm</th>\n",
       "      <th>pe</th>\n",
       "      <th>pb</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>open_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>low_mean</th>\n",
       "      <th>volume_mean</th>\n",
       "      <th>money_mean</th>\n",
       "      <th>high_max</th>\n",
       "      <th>volume_max</th>\n",
       "      <th>money_max</th>\n",
       "      <th>low_min</th>\n",
       "      <th>price_diff</th>\n",
       "      <th>price_diff_max</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-2.352437e-15</td>\n",
       "      <td>-0.058139</td>\n",
       "      <td>-0.050819</td>\n",
       "      <td>-0.019765</td>\n",
       "      <td>0.061225</td>\n",
       "      <td>-0.149061</td>\n",
       "      <td>-0.142580</td>\n",
       "      <td>-0.035876</td>\n",
       "      <td>0.036396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036384</td>\n",
       "      <td>-0.106283</td>\n",
       "      <td>-0.009172</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>-0.085505</td>\n",
       "      <td>-0.011931</td>\n",
       "      <td>0.036343</td>\n",
       "      <td>-0.010037</td>\n",
       "      <td>-0.007684</td>\n",
       "      <td>-0.000584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>-2.352437e-15</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.332743</td>\n",
       "      <td>-0.138436</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>-0.063427</td>\n",
       "      <td>-0.113540</td>\n",
       "      <td>-0.110320</td>\n",
       "      <td>-0.029654</td>\n",
       "      <td>0.074004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074092</td>\n",
       "      <td>-0.210703</td>\n",
       "      <td>-0.136638</td>\n",
       "      <td>0.073257</td>\n",
       "      <td>-0.168088</td>\n",
       "      <td>-0.119024</td>\n",
       "      <td>0.074878</td>\n",
       "      <td>-0.269589</td>\n",
       "      <td>-0.256722</td>\n",
       "      <td>-0.039767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turnoverRatio</th>\n",
       "      <td>-5.813918e-02</td>\n",
       "      <td>-3.327434e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.620331</td>\n",
       "      <td>-0.013631</td>\n",
       "      <td>0.047941</td>\n",
       "      <td>0.119004</td>\n",
       "      <td>0.193394</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>-0.101944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102073</td>\n",
       "      <td>0.598869</td>\n",
       "      <td>0.445796</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>0.537153</td>\n",
       "      <td>0.453859</td>\n",
       "      <td>-0.103462</td>\n",
       "      <td>0.461232</td>\n",
       "      <td>0.442833</td>\n",
       "      <td>-0.009228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transactionAmount</th>\n",
       "      <td>-5.081882e-02</td>\n",
       "      <td>-1.384365e-01</td>\n",
       "      <td>0.620331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007899</td>\n",
       "      <td>0.067887</td>\n",
       "      <td>0.175861</td>\n",
       "      <td>0.179436</td>\n",
       "      <td>-0.001227</td>\n",
       "      <td>0.079523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079379</td>\n",
       "      <td>0.821594</td>\n",
       "      <td>0.771382</td>\n",
       "      <td>0.081019</td>\n",
       "      <td>0.678574</td>\n",
       "      <td>0.695656</td>\n",
       "      <td>0.077892</td>\n",
       "      <td>0.342178</td>\n",
       "      <td>0.318609</td>\n",
       "      <td>-0.037591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pe_ttm</th>\n",
       "      <td>-1.976487e-02</td>\n",
       "      <td>1.253483e-02</td>\n",
       "      <td>-0.013631</td>\n",
       "      <td>-0.007899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031536</td>\n",
       "      <td>-0.040169</td>\n",
       "      <td>-0.013604</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>-0.013801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013794</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.008811</td>\n",
       "      <td>-0.013853</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>-0.006655</td>\n",
       "      <td>-0.013745</td>\n",
       "      <td>-0.017125</td>\n",
       "      <td>-0.018736</td>\n",
       "      <td>-0.002345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pe</th>\n",
       "      <td>6.122505e-02</td>\n",
       "      <td>-6.342674e-02</td>\n",
       "      <td>0.047941</td>\n",
       "      <td>0.067887</td>\n",
       "      <td>0.031536</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011460</td>\n",
       "      <td>0.123659</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000975</td>\n",
       "      <td>0.024455</td>\n",
       "      <td>0.082708</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>0.016186</td>\n",
       "      <td>0.063952</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>0.022307</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>-0.004527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pb</th>\n",
       "      <td>-1.490610e-01</td>\n",
       "      <td>-1.135399e-01</td>\n",
       "      <td>0.119004</td>\n",
       "      <td>0.175861</td>\n",
       "      <td>-0.040169</td>\n",
       "      <td>0.011460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697802</td>\n",
       "      <td>0.014502</td>\n",
       "      <td>0.246572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246482</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.274631</td>\n",
       "      <td>0.247321</td>\n",
       "      <td>-0.026313</td>\n",
       "      <td>0.216644</td>\n",
       "      <td>0.245706</td>\n",
       "      <td>0.089705</td>\n",
       "      <td>0.071745</td>\n",
       "      <td>-0.019484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps</th>\n",
       "      <td>-1.425797e-01</td>\n",
       "      <td>-1.103197e-01</td>\n",
       "      <td>0.193394</td>\n",
       "      <td>0.179436</td>\n",
       "      <td>-0.013604</td>\n",
       "      <td>0.123659</td>\n",
       "      <td>0.697802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010966</td>\n",
       "      <td>0.300355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300288</td>\n",
       "      <td>-0.038059</td>\n",
       "      <td>0.305969</td>\n",
       "      <td>0.300921</td>\n",
       "      <td>-0.040860</td>\n",
       "      <td>0.244450</td>\n",
       "      <td>0.299712</td>\n",
       "      <td>0.065515</td>\n",
       "      <td>0.050696</td>\n",
       "      <td>-0.015925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcf</th>\n",
       "      <td>-3.587599e-02</td>\n",
       "      <td>-2.965431e-02</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>-0.001227</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.014502</td>\n",
       "      <td>-0.010966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.006425</td>\n",
       "      <td>-0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_mean</th>\n",
       "      <td>3.639574e-02</td>\n",
       "      <td>7.400429e-02</td>\n",
       "      <td>-0.101944</td>\n",
       "      <td>0.079523</td>\n",
       "      <td>-0.013801</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.246572</td>\n",
       "      <td>0.300355</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.106221</td>\n",
       "      <td>0.560167</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>-0.090130</td>\n",
       "      <td>0.423446</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>-0.067668</td>\n",
       "      <td>-0.082402</td>\n",
       "      <td>-0.000957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close_mean</th>\n",
       "      <td>3.639624e-02</td>\n",
       "      <td>7.400073e-02</td>\n",
       "      <td>-0.101931</td>\n",
       "      <td>0.079532</td>\n",
       "      <td>-0.013801</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.246579</td>\n",
       "      <td>0.300359</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.106213</td>\n",
       "      <td>0.560187</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>-0.090120</td>\n",
       "      <td>0.423469</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>-0.082397</td>\n",
       "      <td>-0.000958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_mean</th>\n",
       "      <td>3.640754e-02</td>\n",
       "      <td>7.391315e-02</td>\n",
       "      <td>-0.101804</td>\n",
       "      <td>0.079676</td>\n",
       "      <td>-0.013807</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>0.246665</td>\n",
       "      <td>0.300425</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>-0.106163</td>\n",
       "      <td>0.560401</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>-0.090086</td>\n",
       "      <td>0.423649</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>-0.067405</td>\n",
       "      <td>-0.082139</td>\n",
       "      <td>-0.000974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low_mean</th>\n",
       "      <td>3.638398e-02</td>\n",
       "      <td>7.409183e-02</td>\n",
       "      <td>-0.102073</td>\n",
       "      <td>0.079379</td>\n",
       "      <td>-0.013794</td>\n",
       "      <td>-0.000975</td>\n",
       "      <td>0.246482</td>\n",
       "      <td>0.300288</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.106273</td>\n",
       "      <td>0.559954</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>-0.090165</td>\n",
       "      <td>0.423271</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>-0.067893</td>\n",
       "      <td>-0.082658</td>\n",
       "      <td>-0.000940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume_mean</th>\n",
       "      <td>-1.062829e-01</td>\n",
       "      <td>-2.107028e-01</td>\n",
       "      <td>0.598869</td>\n",
       "      <td>0.821594</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.024455</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>-0.038059</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>-0.106221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.532972</td>\n",
       "      <td>-0.105627</td>\n",
       "      <td>0.884418</td>\n",
       "      <td>0.534642</td>\n",
       "      <td>-0.106886</td>\n",
       "      <td>0.362313</td>\n",
       "      <td>0.341582</td>\n",
       "      <td>-0.007264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money_mean</th>\n",
       "      <td>-9.172001e-03</td>\n",
       "      <td>-1.366378e-01</td>\n",
       "      <td>0.445796</td>\n",
       "      <td>0.771382</td>\n",
       "      <td>-0.008811</td>\n",
       "      <td>0.082708</td>\n",
       "      <td>0.274631</td>\n",
       "      <td>0.305969</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.560167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559954</td>\n",
       "      <td>0.532972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.452106</td>\n",
       "      <td>0.881190</td>\n",
       "      <td>0.557258</td>\n",
       "      <td>0.246143</td>\n",
       "      <td>0.213686</td>\n",
       "      <td>-0.020659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_max</th>\n",
       "      <td>3.644275e-02</td>\n",
       "      <td>7.325713e-02</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>0.081019</td>\n",
       "      <td>-0.013853</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>0.247321</td>\n",
       "      <td>0.300921</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>-0.105627</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.089643</td>\n",
       "      <td>0.425509</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>-0.065377</td>\n",
       "      <td>-0.079808</td>\n",
       "      <td>-0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume_max</th>\n",
       "      <td>-8.550527e-02</td>\n",
       "      <td>-1.680876e-01</td>\n",
       "      <td>0.537153</td>\n",
       "      <td>0.678574</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.016186</td>\n",
       "      <td>-0.026313</td>\n",
       "      <td>-0.040860</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>-0.090130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090165</td>\n",
       "      <td>0.884418</td>\n",
       "      <td>0.452106</td>\n",
       "      <td>-0.089643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597230</td>\n",
       "      <td>-0.090693</td>\n",
       "      <td>0.374550</td>\n",
       "      <td>0.357065</td>\n",
       "      <td>-0.002709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money_max</th>\n",
       "      <td>-1.193068e-02</td>\n",
       "      <td>-1.190244e-01</td>\n",
       "      <td>0.453859</td>\n",
       "      <td>0.695656</td>\n",
       "      <td>-0.006655</td>\n",
       "      <td>0.063952</td>\n",
       "      <td>0.216644</td>\n",
       "      <td>0.244450</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.423446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423271</td>\n",
       "      <td>0.534642</td>\n",
       "      <td>0.881190</td>\n",
       "      <td>0.425509</td>\n",
       "      <td>0.597230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421126</td>\n",
       "      <td>0.290578</td>\n",
       "      <td>0.263970</td>\n",
       "      <td>-0.012619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low_min</th>\n",
       "      <td>3.634272e-02</td>\n",
       "      <td>7.487789e-02</td>\n",
       "      <td>-0.103462</td>\n",
       "      <td>0.077892</td>\n",
       "      <td>-0.013745</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>0.245706</td>\n",
       "      <td>0.299712</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>-0.106886</td>\n",
       "      <td>0.557258</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>-0.090693</td>\n",
       "      <td>0.421126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.070524</td>\n",
       "      <td>-0.085278</td>\n",
       "      <td>-0.000981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_diff</th>\n",
       "      <td>-1.003678e-02</td>\n",
       "      <td>-2.695888e-01</td>\n",
       "      <td>0.461232</td>\n",
       "      <td>0.342178</td>\n",
       "      <td>-0.017125</td>\n",
       "      <td>0.022307</td>\n",
       "      <td>0.089705</td>\n",
       "      <td>0.065515</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>-0.067668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067893</td>\n",
       "      <td>0.362313</td>\n",
       "      <td>0.246143</td>\n",
       "      <td>-0.065377</td>\n",
       "      <td>0.374550</td>\n",
       "      <td>0.290578</td>\n",
       "      <td>-0.070524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854131</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_diff_max</th>\n",
       "      <td>-7.683573e-03</td>\n",
       "      <td>-2.567217e-01</td>\n",
       "      <td>0.442833</td>\n",
       "      <td>0.318609</td>\n",
       "      <td>-0.018736</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.071745</td>\n",
       "      <td>0.050696</td>\n",
       "      <td>0.006425</td>\n",
       "      <td>-0.082402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082658</td>\n",
       "      <td>0.341582</td>\n",
       "      <td>0.213686</td>\n",
       "      <td>-0.079808</td>\n",
       "      <td>0.357065</td>\n",
       "      <td>0.263970</td>\n",
       "      <td>-0.085278</td>\n",
       "      <td>0.854131</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return</th>\n",
       "      <td>-5.837684e-04</td>\n",
       "      <td>-3.976677e-02</td>\n",
       "      <td>-0.009228</td>\n",
       "      <td>-0.037591</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>-0.004527</td>\n",
       "      <td>-0.019484</td>\n",
       "      <td>-0.015925</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>-0.000957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>-0.007264</td>\n",
       "      <td>-0.020659</td>\n",
       "      <td>-0.001244</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>-0.012619</td>\n",
       "      <td>-0.000981</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       stock_id           day  turnoverRatio  \\\n",
       "stock_id           1.000000e+00 -2.352437e-15      -0.058139   \n",
       "day               -2.352437e-15  1.000000e+00      -0.332743   \n",
       "turnoverRatio     -5.813918e-02 -3.327434e-01       1.000000   \n",
       "transactionAmount -5.081882e-02 -1.384365e-01       0.620331   \n",
       "pe_ttm            -1.976487e-02  1.253483e-02      -0.013631   \n",
       "pe                 6.122505e-02 -6.342674e-02       0.047941   \n",
       "pb                -1.490610e-01 -1.135399e-01       0.119004   \n",
       "ps                -1.425797e-01 -1.103197e-01       0.193394   \n",
       "pcf               -3.587599e-02 -2.965431e-02       0.005828   \n",
       "open_mean          3.639574e-02  7.400429e-02      -0.101944   \n",
       "close_mean         3.639624e-02  7.400073e-02      -0.101931   \n",
       "high_mean          3.640754e-02  7.391315e-02      -0.101804   \n",
       "low_mean           3.638398e-02  7.409183e-02      -0.102073   \n",
       "volume_mean       -1.062829e-01 -2.107028e-01       0.598869   \n",
       "money_mean        -9.172001e-03 -1.366378e-01       0.445796   \n",
       "high_max           3.644275e-02  7.325713e-02      -0.100592   \n",
       "volume_max        -8.550527e-02 -1.680876e-01       0.537153   \n",
       "money_max         -1.193068e-02 -1.190244e-01       0.453859   \n",
       "low_min            3.634272e-02  7.487789e-02      -0.103462   \n",
       "price_diff        -1.003678e-02 -2.695888e-01       0.461232   \n",
       "price_diff_max    -7.683573e-03 -2.567217e-01       0.442833   \n",
       "return            -5.837684e-04 -3.976677e-02      -0.009228   \n",
       "\n",
       "                   transactionAmount    pe_ttm        pe        pb        ps  \\\n",
       "stock_id                   -0.050819 -0.019765  0.061225 -0.149061 -0.142580   \n",
       "day                        -0.138436  0.012535 -0.063427 -0.113540 -0.110320   \n",
       "turnoverRatio               0.620331 -0.013631  0.047941  0.119004  0.193394   \n",
       "transactionAmount           1.000000 -0.007899  0.067887  0.175861  0.179436   \n",
       "pe_ttm                     -0.007899  1.000000  0.031536 -0.040169 -0.013604   \n",
       "pe                          0.067887  0.031536  1.000000  0.011460  0.123659   \n",
       "pb                          0.175861 -0.040169  0.011460  1.000000  0.697802   \n",
       "ps                          0.179436 -0.013604  0.123659  0.697802  1.000000   \n",
       "pcf                        -0.001227  0.003151  0.003921  0.014502 -0.010966   \n",
       "open_mean                   0.079523 -0.013801 -0.000937  0.246572  0.300355   \n",
       "close_mean                  0.079532 -0.013801 -0.000937  0.246579  0.300359   \n",
       "high_mean                   0.079676 -0.013807 -0.000901  0.246665  0.300425   \n",
       "low_mean                    0.079379 -0.013794 -0.000975  0.246482  0.300288   \n",
       "volume_mean                 0.821594 -0.000013  0.024455 -0.015220 -0.038059   \n",
       "money_mean                  0.771382 -0.008811  0.082708  0.274631  0.305969   \n",
       "high_max                    0.081019 -0.013853 -0.000602  0.247321  0.300921   \n",
       "volume_max                  0.678574  0.000824  0.016186 -0.026313 -0.040860   \n",
       "money_max                   0.695656 -0.006655  0.063952  0.216644  0.244450   \n",
       "low_min                     0.077892 -0.013745 -0.001270  0.245706  0.299712   \n",
       "price_diff                  0.342178 -0.017125  0.022307  0.089705  0.065515   \n",
       "price_diff_max              0.318609 -0.018736  0.021626  0.071745  0.050696   \n",
       "return                     -0.037591 -0.002345 -0.004527 -0.019484 -0.015925   \n",
       "\n",
       "                        pcf  open_mean  ...  low_mean  volume_mean  \\\n",
       "stock_id          -0.035876   0.036396  ...  0.036384    -0.106283   \n",
       "day               -0.029654   0.074004  ...  0.074092    -0.210703   \n",
       "turnoverRatio      0.005828  -0.101944  ... -0.102073     0.598869   \n",
       "transactionAmount -0.001227   0.079523  ...  0.079379     0.821594   \n",
       "pe_ttm             0.003151  -0.013801  ... -0.013794    -0.000013   \n",
       "pe                 0.003921  -0.000937  ... -0.000975     0.024455   \n",
       "pb                 0.014502   0.246572  ...  0.246482    -0.015220   \n",
       "ps                -0.010966   0.300355  ...  0.300288    -0.038059   \n",
       "pcf                1.000000   0.001034  ...  0.001034     0.007004   \n",
       "open_mean          0.001034   1.000000  ...  1.000000    -0.106221   \n",
       "close_mean         0.001034   1.000000  ...  1.000000    -0.106213   \n",
       "high_mean          0.001033   1.000000  ...  0.999999    -0.106163   \n",
       "low_mean           0.001034   1.000000  ...  1.000000    -0.106273   \n",
       "volume_mean        0.007004  -0.106221  ... -0.106273     1.000000   \n",
       "money_mean         0.003289   0.560167  ...  0.559954     0.532972   \n",
       "high_max           0.001056   0.999965  ...  0.999961    -0.105627   \n",
       "volume_max         0.005662  -0.090130  ... -0.090165     0.884418   \n",
       "money_max          0.003112   0.423446  ...  0.423271     0.534642   \n",
       "low_min            0.001023   0.999954  ...  0.999958    -0.106886   \n",
       "price_diff         0.005412  -0.067668  ... -0.067893     0.362313   \n",
       "price_diff_max     0.006425  -0.082402  ... -0.082658     0.341582   \n",
       "return            -0.000343  -0.000957  ... -0.000940    -0.007264   \n",
       "\n",
       "                   money_mean  high_max  volume_max  money_max   low_min  \\\n",
       "stock_id            -0.009172  0.036443   -0.085505  -0.011931  0.036343   \n",
       "day                 -0.136638  0.073257   -0.168088  -0.119024  0.074878   \n",
       "turnoverRatio        0.445796 -0.100592    0.537153   0.453859 -0.103462   \n",
       "transactionAmount    0.771382  0.081019    0.678574   0.695656  0.077892   \n",
       "pe_ttm              -0.008811 -0.013853    0.000824  -0.006655 -0.013745   \n",
       "pe                   0.082708 -0.000602    0.016186   0.063952 -0.001270   \n",
       "pb                   0.274631  0.247321   -0.026313   0.216644  0.245706   \n",
       "ps                   0.305969  0.300921   -0.040860   0.244450  0.299712   \n",
       "pcf                  0.003289  0.001056    0.005662   0.003112  0.001023   \n",
       "open_mean            0.560167  0.999965   -0.090130   0.423446  0.999954   \n",
       "close_mean           0.560187  0.999964   -0.090120   0.423469  0.999953   \n",
       "high_mean            0.560401  0.999969   -0.090086   0.423649  0.999949   \n",
       "low_mean             0.559954  0.999961   -0.090165   0.423271  0.999958   \n",
       "volume_mean          0.532972 -0.105627    0.884418   0.534642 -0.106886   \n",
       "money_mean           1.000000  0.562600    0.452106   0.881190  0.557258   \n",
       "high_max             0.562600  1.000000   -0.089643   0.425509  0.999877   \n",
       "volume_max           0.452106 -0.089643    1.000000   0.597230 -0.090693   \n",
       "money_max            0.881190  0.425509    0.597230   1.000000  0.421126   \n",
       "low_min              0.557258  0.999877   -0.090693   0.421126  1.000000   \n",
       "price_diff           0.246143 -0.065377    0.374550   0.290578 -0.070524   \n",
       "price_diff_max       0.213686 -0.079808    0.357065   0.263970 -0.085278   \n",
       "return              -0.020659 -0.001244   -0.002709  -0.012619 -0.000981   \n",
       "\n",
       "                   price_diff  price_diff_max    return  \n",
       "stock_id            -0.010037       -0.007684 -0.000584  \n",
       "day                 -0.269589       -0.256722 -0.039767  \n",
       "turnoverRatio        0.461232        0.442833 -0.009228  \n",
       "transactionAmount    0.342178        0.318609 -0.037591  \n",
       "pe_ttm              -0.017125       -0.018736 -0.002345  \n",
       "pe                   0.022307        0.021626 -0.004527  \n",
       "pb                   0.089705        0.071745 -0.019484  \n",
       "ps                   0.065515        0.050696 -0.015925  \n",
       "pcf                  0.005412        0.006425 -0.000343  \n",
       "open_mean           -0.067668       -0.082402 -0.000957  \n",
       "close_mean          -0.067627       -0.082397 -0.000958  \n",
       "high_mean           -0.067405       -0.082139 -0.000974  \n",
       "low_mean            -0.067893       -0.082658 -0.000940  \n",
       "volume_mean          0.362313        0.341582 -0.007264  \n",
       "money_mean           0.246143        0.213686 -0.020659  \n",
       "high_max            -0.065377       -0.079808 -0.001244  \n",
       "volume_max           0.374550        0.357065 -0.002709  \n",
       "money_max            0.290578        0.263970 -0.012619  \n",
       "low_min             -0.070524       -0.085278 -0.000981  \n",
       "price_diff           1.000000        0.854131  0.001331  \n",
       "price_diff_max       0.854131        1.000000 -0.000914  \n",
       "return               0.001331       -0.000914  1.000000  \n",
       "\n",
       "[22 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([train,valid],axis=0)\n",
    "\n",
    "train_y = train[\"return\"]\n",
    "train = train.drop(columns=[\"return\", \"date_time\",'stock_id','day'])\n",
    "# train = train.iloc[:, [4,14,15,17,18]]\n",
    "# train = train.drop(columns=[\"return\", \"date_time\",'stock_id'])\n",
    "# train = train.iloc[:, [4,14,15,17,18,21,22,23,24,25]]\n",
    "\n",
    "valid_y = valid[\"return\"]\n",
    "valid = valid.drop(columns=[\"return\", \"date_time\"])\n",
    "\n",
    "test = test.drop(columns=[\"date_time\",'stock_id','day'])\n",
    "# test = test.iloc[:, [4,14,15,17,18]]\n",
    "# test = test.drop(columns=[\"date_time\",'stock_id'])\n",
    "# test = test.iloc[:, [4,14,15,17,18,21,22,23,24,25]]\n",
    "\n",
    "real_return = real_return.drop(columns=[\"date_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lst = []\n",
    "train_y_lst = []\n",
    "test_lst = []\n",
    "real_y_lst = []\n",
    "for i in range(54):\n",
    "    idx1 = 998 * i \n",
    "    idx2 = 998 * (i + 1)\n",
    "    test_idx1 = 700 * i\n",
    "    test_idx2 = 700 * (i+1)\n",
    "    return_idx1 = 698 * i\n",
    "    return_idx2 = 698 * (i + 1)\n",
    "    train_lst.append(train.iloc[idx1:idx2])\n",
    "    train_y_lst.append(train_y.iloc[idx1:idx2].reset_index()[\"return\"])\n",
    "    test_lst.append(test.iloc[test_idx1:test_idx2])\n",
    "    real_y_lst.append(real_return.iloc[return_idx1:return_idx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.002691\n",
       "1      0.030722\n",
       "2      0.043125\n",
       "3      0.032530\n",
       "4     -0.028420\n",
       "         ...   \n",
       "993   -0.006676\n",
       "994   -0.030473\n",
       "995   -0.012935\n",
       "996   -0.003142\n",
       "997   -0.006291\n",
       "Name: return, Length: 998, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_lst[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respective training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training correlation 0.259631000985571\n",
      "testing correlation 0.03600819140108404\n",
      "training correlation 0.18351614435354574\n",
      "testing correlation 0.0003338814657974809\n",
      "training correlation 0.17301410984593485\n",
      "testing correlation 0.0026657016443473198\n",
      "training correlation 0.16876284512114756\n",
      "testing correlation -0.0002846501887429731\n",
      "training correlation 0.1662745079281748\n",
      "testing correlation -0.0057009441430388936\n",
      "training correlation 0.16291643862518673\n",
      "testing correlation -0.01141634212300197\n",
      "training correlation 0.15866617927212487\n",
      "testing correlation -0.017439416131668387\n",
      "training correlation 0.15819535024417974\n",
      "testing correlation -0.019145590444842965\n",
      "training correlation 0.1579929842524283\n",
      "testing correlation -0.020253457629916212\n",
      "training correlation 0.15776572364286107\n",
      "testing correlation -0.021375551822930345\n",
      "training correlation 0.15744458893690635\n",
      "testing correlation -0.022793760931803183\n",
      "training correlation 0.15708346085990768\n",
      "testing correlation -0.024220671787161138\n",
      "training correlation 0.1566813960893575\n",
      "testing correlation -0.025653188813221307\n",
      "training correlation 0.15623707831747075\n",
      "testing correlation -0.02708948592655467\n",
      "training correlation 0.15574994847017648\n",
      "testing correlation -0.02852529704773405\n",
      "training correlation 0.15521892596646195\n",
      "testing correlation -0.029958136994279558\n",
      "training correlation 0.15464335211079272\n",
      "testing correlation -0.031384415837487856\n",
      "training correlation 0.1540225473981561\n",
      "testing correlation -0.032800814861571596\n",
      "training correlation 0.15335544034005577\n",
      "testing correlation -0.034204962614238436\n",
      "training correlation 0.1526423655346376\n",
      "testing correlation -0.03559180096647867\n",
      "training correlation 0.15188233152023575\n",
      "testing correlation -0.03695925863092109\n",
      "training correlation 0.15105358736340524\n",
      "testing correlation -0.0383379609053322\n",
      "training correlation 0.15018764883987662\n",
      "testing correlation -0.03967095050893466\n",
      "training correlation 0.14928369161312516\n",
      "testing correlation -0.04096023122186018\n",
      "training correlation 0.14857051756410997\n",
      "testing correlation -0.04187736828414863\n",
      "training correlation 0.1485408490166892\n",
      "testing correlation -0.04177973178977032\n",
      "training correlation 0.1485094673472849\n",
      "testing correlation -0.04168097428799083\n",
      "training correlation 0.14785517503187995\n",
      "testing correlation -0.042453004828139886\n",
      "training correlation 0.14692330221735506\n",
      "testing correlation -0.0435457948968427\n",
      "training correlation 0.14652200202986657\n",
      "testing correlation -0.043991324077150076\n",
      "training correlation 0.1465219567391759\n",
      "testing correlation -0.04398689991888298\n",
      "training correlation 0.14652190950581623\n",
      "testing correlation -0.043982481031188084\n",
      "training correlation 0.14652186033450862\n",
      "testing correlation -0.043978061046379364\n",
      "training correlation 0.1465218093544852\n",
      "testing correlation -0.0439736174162415\n",
      "training correlation 0.14652175636076534\n",
      "testing correlation -0.043969175077626516\n",
      "training correlation 0.1465217017703765\n",
      "testing correlation -0.043964676641058055\n",
      "training correlation 0.1465216448934533\n",
      "testing correlation -0.04396020741502829\n",
      "training correlation 0.1465215864267888\n",
      "testing correlation -0.04395567862828907\n",
      "training correlation 0.1465215256466134\n",
      "testing correlation -0.04395117328933742\n",
      "training correlation 0.1465214632328176\n",
      "testing correlation -0.04394661151063785\n",
      "training correlation 0.14652139883642798\n",
      "testing correlation -0.04394203122639297\n",
      "training correlation 0.14652133185531055\n",
      "testing correlation -0.04393749010178193\n",
      "training correlation 0.14652126359897247\n",
      "testing correlation -0.043932854402955045\n",
      "training correlation 0.1465211928918197\n",
      "testing correlation -0.0439282385624139\n",
      "training correlation 0.1465211196695586\n",
      "testing correlation -0.0439236422305835\n",
      "training correlation 0.1465210447000282\n",
      "testing correlation -0.04391899344204043\n",
      "training correlation 0.1465209680770801\n",
      "testing correlation -0.043914285847938994\n",
      "training correlation 0.14652088907987135\n",
      "testing correlation -0.04390957926789395\n",
      "training correlation 0.14652080755233332\n",
      "testing correlation -0.04390488201231041\n",
      "training correlation 0.1465207235780919\n",
      "testing correlation -0.04390018313554955\n",
      "training correlation 0.1465206380660476\n",
      "testing correlation -0.04389541326728228\n",
      "training correlation 0.14652055013239149\n",
      "testing correlation -0.0438906374996488\n",
      "training correlation 0.1465204596914549\n",
      "testing correlation -0.043885858665910694\n",
      "training correlation 0.14652036671180274\n",
      "testing correlation -0.043881075560779394\n",
      "training correlation 0.14652027116173214\n",
      "testing correlation -0.04387628702157388\n",
      "training correlation 0.14652017429744676\n",
      "testing correlation -0.04387141271169826\n",
      "training correlation 0.14652007488507832\n",
      "testing correlation -0.04386653062975786\n",
      "training correlation 0.14651997159507713\n",
      "testing correlation -0.04386171510355277\n",
      "training correlation 0.14651986695197144\n",
      "testing correlation -0.0438568150830188\n",
      "training correlation 0.14651975967100736\n",
      "testing correlation -0.043851904598791\n",
      "training correlation 0.14651965133235315\n",
      "testing correlation -0.04384689787210842\n",
      "training correlation 0.1465195388678455\n",
      "testing correlation -0.04384195729500175\n",
      "training correlation 0.1465194235325853\n",
      "testing correlation -0.043837011463669834\n",
      "training correlation 0.14651930733903928\n",
      "testing correlation -0.04383196040971594\n",
      "training correlation 0.14651918651920182\n",
      "testing correlation -0.043826986669004524\n",
      "training correlation 0.14651906483960733\n",
      "testing correlation -0.043821908918422185\n",
      "training correlation 0.14651893852661868\n",
      "testing correlation -0.043816899764648184\n",
      "training correlation 0.1465188111573951\n",
      "testing correlation -0.04381179654602413\n",
      "training correlation 0.14651868106237442\n",
      "testing correlation -0.04380667369765896\n",
      "training correlation 0.14651854817422796\n",
      "testing correlation -0.04380153253690163\n",
      "training correlation 0.14651841242489058\n",
      "testing correlation -0.04379637423425196\n",
      "training correlation 0.14651827590929947\n",
      "testing correlation -0.04379111649878817\n",
      "training correlation 0.1465181343621528\n",
      "testing correlation -0.04378592423695502\n",
      "training correlation 0.146517989846855\n",
      "testing correlation -0.043780714060903066\n",
      "training correlation 0.14651784231458354\n",
      "testing correlation -0.043775486150404316\n",
      "training correlation 0.14651769409315324\n",
      "testing correlation -0.04377015850501373\n",
      "training correlation 0.14651754051893845\n",
      "testing correlation -0.04376489301755389\n",
      "training correlation 0.1465173838255295\n",
      "testing correlation -0.043759608729700084\n",
      "training correlation 0.14651722651106103\n",
      "testing correlation -0.04375422452403926\n",
      "training correlation 0.1465170635867716\n",
      "testing correlation -0.043748900243807017\n",
      "training correlation 0.1465169001522449\n",
      "testing correlation -0.0437434741157279\n",
      "training correlation 0.1465167309519849\n",
      "testing correlation -0.04373810602571039\n",
      "training correlation 0.14651656104454147\n",
      "testing correlation -0.04373264345159411\n",
      "training correlation 0.14651638518386556\n",
      "testing correlation -0.04372723749509508\n",
      "training correlation 0.14651620889506478\n",
      "testing correlation -0.04372173048815551\n",
      "training correlation 0.1465160264809562\n",
      "testing correlation -0.043716278470181696\n",
      "training correlation 0.14651584342384594\n",
      "testing correlation -0.043710732722718054\n",
      "training correlation 0.146515657071973\n",
      "testing correlation -0.043705164296301044\n",
      "training correlation 0.14651546443878252\n",
      "testing correlation -0.04369964547594016\n",
      "training correlation 0.14651527116814994\n",
      "testing correlation -0.043694034693235885\n",
      "training correlation 0.1465150713605066\n",
      "testing correlation -0.04368847352153232\n",
      "training correlation 0.1465148712005789\n",
      "testing correlation -0.04368281501982768\n",
      "training correlation 0.14651466735095928\n",
      "testing correlation -0.04367713750042574\n",
      "training correlation 0.1465144601371371\n",
      "testing correlation -0.043671432617899617\n",
      "training correlation 0.14651424569036745\n",
      "testing correlation -0.04366578097533006\n",
      "training correlation 0.14651403107156047\n",
      "testing correlation -0.043660031146806375\n",
      "training correlation 0.14651381265382274\n",
      "testing correlation -0.043654260049497705\n",
      "training correlation 0.14651358690208988\n",
      "testing correlation -0.043648535243008976\n",
      "training correlation 0.146513360974349\n",
      "testing correlation -0.04364271479640979\n",
      "training correlation 0.14651313109202144\n",
      "testing correlation -0.043636872812151534\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "model = LinearRegression()\n",
    "# for i in range(54):\n",
    "    # model = linear_model.Lasso(alpha=i)\n",
    "\n",
    "for i in range(100):\n",
    "    # model = make_pipeline(StandardScaler(),SGDRegressor(max_iter=10000, tol=1e-6,penalty='l1',random_state=i))\n",
    "    model = linear_model.Lasso(alpha=i)\n",
    "    pred_temp, score, fit_y, train_y = evaluate4(model, train_lst[1], test_lst[1], train_y_lst[1], real_y_lst[1])\n",
    "    print(\"testing correlation\",score)\n",
    "# model_lst.append(LinearRegression())\n",
    "# print(fit_y)\n",
    "# print(train_y)\n",
    "    # pred_temp, score = evaluate2(model, train_lst[2], test_lst[2], train_y_lst[2], real_y_lst[2])\n",
    "\n",
    "# model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turnoverRatio</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>pe_ttm</th>\n",
       "      <th>pe</th>\n",
       "      <th>pb</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>open_mean</th>\n",
       "      <th>close_mean</th>\n",
       "      <th>high_mean</th>\n",
       "      <th>low_mean</th>\n",
       "      <th>volume_mean</th>\n",
       "      <th>money_mean</th>\n",
       "      <th>high_max</th>\n",
       "      <th>volume_max</th>\n",
       "      <th>money_max</th>\n",
       "      <th>low_min</th>\n",
       "      <th>price_diff</th>\n",
       "      <th>price_diff_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27300</th>\n",
       "      <td>0.4676</td>\n",
       "      <td>2752.0</td>\n",
       "      <td>19.1125</td>\n",
       "      <td>19.1125</td>\n",
       "      <td>1.6078</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>20.1073</td>\n",
       "      <td>3.879068</td>\n",
       "      <td>3.881978</td>\n",
       "      <td>3.898496</td>\n",
       "      <td>3.871780</td>\n",
       "      <td>87486.72</td>\n",
       "      <td>3.410831e+05</td>\n",
       "      <td>4.0541</td>\n",
       "      <td>469366.0</td>\n",
       "      <td>1.852525e+06</td>\n",
       "      <td>3.8356</td>\n",
       "      <td>0.021748</td>\n",
       "      <td>0.056608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27301</th>\n",
       "      <td>0.4790</td>\n",
       "      <td>2296.0</td>\n",
       "      <td>19.1682</td>\n",
       "      <td>19.1682</td>\n",
       "      <td>1.6124</td>\n",
       "      <td>0.8760</td>\n",
       "      <td>20.1660</td>\n",
       "      <td>3.972776</td>\n",
       "      <td>3.973502</td>\n",
       "      <td>3.979092</td>\n",
       "      <td>3.961824</td>\n",
       "      <td>89619.00</td>\n",
       "      <td>3.563110e+05</td>\n",
       "      <td>4.0298</td>\n",
       "      <td>348760.0</td>\n",
       "      <td>1.391282e+06</td>\n",
       "      <td>3.9327</td>\n",
       "      <td>0.009253</td>\n",
       "      <td>0.012108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27302</th>\n",
       "      <td>0.4926</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>19.3354</td>\n",
       "      <td>19.3354</td>\n",
       "      <td>1.6265</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>20.3418</td>\n",
       "      <td>4.000688</td>\n",
       "      <td>4.000438</td>\n",
       "      <td>4.009914</td>\n",
       "      <td>3.990478</td>\n",
       "      <td>92171.08</td>\n",
       "      <td>3.691468e+05</td>\n",
       "      <td>4.0541</td>\n",
       "      <td>516041.0</td>\n",
       "      <td>2.074592e+06</td>\n",
       "      <td>3.9570</td>\n",
       "      <td>0.018285</td>\n",
       "      <td>0.018285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27303</th>\n",
       "      <td>0.5093</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>19.4468</td>\n",
       "      <td>19.4468</td>\n",
       "      <td>1.6359</td>\n",
       "      <td>0.8887</td>\n",
       "      <td>20.4591</td>\n",
       "      <td>4.027402</td>\n",
       "      <td>4.029578</td>\n",
       "      <td>4.037838</td>\n",
       "      <td>4.021334</td>\n",
       "      <td>95282.88</td>\n",
       "      <td>3.845482e+05</td>\n",
       "      <td>4.0662</td>\n",
       "      <td>317225.0</td>\n",
       "      <td>1.270574e+06</td>\n",
       "      <td>3.9813</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.009143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27304</th>\n",
       "      <td>0.3062</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>19.3354</td>\n",
       "      <td>19.3354</td>\n",
       "      <td>1.6265</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>20.3418</td>\n",
       "      <td>4.026176</td>\n",
       "      <td>4.026178</td>\n",
       "      <td>4.031294</td>\n",
       "      <td>4.017426</td>\n",
       "      <td>57282.20</td>\n",
       "      <td>2.305792e+05</td>\n",
       "      <td>4.0541</td>\n",
       "      <td>342600.0</td>\n",
       "      <td>1.376049e+06</td>\n",
       "      <td>4.0055</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.006067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>0.7606</td>\n",
       "      <td>6923.0</td>\n",
       "      <td>12.5212</td>\n",
       "      <td>15.4301</td>\n",
       "      <td>1.5290</td>\n",
       "      <td>0.8723</td>\n",
       "      <td>-18.2304</td>\n",
       "      <td>6.022384</td>\n",
       "      <td>6.023112</td>\n",
       "      <td>6.034534</td>\n",
       "      <td>6.010726</td>\n",
       "      <td>171852.00</td>\n",
       "      <td>1.036837e+06</td>\n",
       "      <td>6.0933</td>\n",
       "      <td>1165100.0</td>\n",
       "      <td>7.063849e+06</td>\n",
       "      <td>5.8991</td>\n",
       "      <td>0.012316</td>\n",
       "      <td>0.012316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>1.0603</td>\n",
       "      <td>9910.0</td>\n",
       "      <td>13.0281</td>\n",
       "      <td>16.0548</td>\n",
       "      <td>1.5909</td>\n",
       "      <td>0.9076</td>\n",
       "      <td>-18.9685</td>\n",
       "      <td>6.175324</td>\n",
       "      <td>6.178484</td>\n",
       "      <td>6.189894</td>\n",
       "      <td>6.161734</td>\n",
       "      <td>239562.00</td>\n",
       "      <td>1.481691e+06</td>\n",
       "      <td>6.2875</td>\n",
       "      <td>1032600.0</td>\n",
       "      <td>6.338871e+06</td>\n",
       "      <td>5.9840</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>1.1782</td>\n",
       "      <td>9576.0</td>\n",
       "      <td>13.0281</td>\n",
       "      <td>16.0548</td>\n",
       "      <td>1.5909</td>\n",
       "      <td>0.9076</td>\n",
       "      <td>-18.9685</td>\n",
       "      <td>6.183098</td>\n",
       "      <td>6.183108</td>\n",
       "      <td>6.196936</td>\n",
       "      <td>6.166840</td>\n",
       "      <td>266202.00</td>\n",
       "      <td>1.651576e+06</td>\n",
       "      <td>6.3118</td>\n",
       "      <td>1282200.0</td>\n",
       "      <td>8.072124e+06</td>\n",
       "      <td>6.0933</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.015594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>0.7219</td>\n",
       "      <td>7769.0</td>\n",
       "      <td>12.5719</td>\n",
       "      <td>15.4926</td>\n",
       "      <td>1.5352</td>\n",
       "      <td>0.8758</td>\n",
       "      <td>-18.3042</td>\n",
       "      <td>6.081130</td>\n",
       "      <td>6.078702</td>\n",
       "      <td>6.091822</td>\n",
       "      <td>6.068994</td>\n",
       "      <td>163100.00</td>\n",
       "      <td>9.926983e+05</td>\n",
       "      <td>6.2147</td>\n",
       "      <td>760400.0</td>\n",
       "      <td>4.684179e+06</td>\n",
       "      <td>5.9840</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>0.013704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>0.4566</td>\n",
       "      <td>6903.0</td>\n",
       "      <td>12.7240</td>\n",
       "      <td>15.6800</td>\n",
       "      <td>1.5538</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>-18.5257</td>\n",
       "      <td>6.077014</td>\n",
       "      <td>6.076772</td>\n",
       "      <td>6.089390</td>\n",
       "      <td>6.065354</td>\n",
       "      <td>103160.00</td>\n",
       "      <td>6.271132e+05</td>\n",
       "      <td>6.1297</td>\n",
       "      <td>413800.0</td>\n",
       "      <td>2.523245e+06</td>\n",
       "      <td>6.0204</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.009982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       turnoverRatio  transactionAmount   pe_ttm       pe      pb      ps  \\\n",
       "27300         0.4676             2752.0  19.1125  19.1125  1.6078  0.8735   \n",
       "27301         0.4790             2296.0  19.1682  19.1682  1.6124  0.8760   \n",
       "27302         0.4926             1923.0  19.3354  19.3354  1.6265  0.8836   \n",
       "27303         0.5093             1518.0  19.4468  19.4468  1.6359  0.8887   \n",
       "27304         0.3062             1131.0  19.3354  19.3354  1.6265  0.8836   \n",
       "...              ...                ...      ...      ...     ...     ...   \n",
       "27995         0.7606             6923.0  12.5212  15.4301  1.5290  0.8723   \n",
       "27996         1.0603             9910.0  13.0281  16.0548  1.5909  0.9076   \n",
       "27997         1.1782             9576.0  13.0281  16.0548  1.5909  0.9076   \n",
       "27998         0.7219             7769.0  12.5719  15.4926  1.5352  0.8758   \n",
       "27999         0.4566             6903.0  12.7240  15.6800  1.5538  0.8864   \n",
       "\n",
       "           pcf  open_mean  close_mean  high_mean  low_mean  volume_mean  \\\n",
       "27300  20.1073   3.879068    3.881978   3.898496  3.871780     87486.72   \n",
       "27301  20.1660   3.972776    3.973502   3.979092  3.961824     89619.00   \n",
       "27302  20.3418   4.000688    4.000438   4.009914  3.990478     92171.08   \n",
       "27303  20.4591   4.027402    4.029578   4.037838  4.021334     95282.88   \n",
       "27304  20.3418   4.026176    4.026178   4.031294  4.017426     57282.20   \n",
       "...        ...        ...         ...        ...       ...          ...   \n",
       "27995 -18.2304   6.022384    6.023112   6.034534  6.010726    171852.00   \n",
       "27996 -18.9685   6.175324    6.178484   6.189894  6.161734    239562.00   \n",
       "27997 -18.9685   6.183098    6.183108   6.196936  6.166840    266202.00   \n",
       "27998 -18.3042   6.081130    6.078702   6.091822  6.068994    163100.00   \n",
       "27999 -18.5257   6.077014    6.076772   6.089390  6.065354    103160.00   \n",
       "\n",
       "         money_mean  high_max  volume_max     money_max  low_min  price_diff  \\\n",
       "27300  3.410831e+05    4.0541    469366.0  1.852525e+06   3.8356    0.021748   \n",
       "27301  3.563110e+05    4.0298    348760.0  1.391282e+06   3.9327    0.009253   \n",
       "27302  3.691468e+05    4.0541    516041.0  2.074592e+06   3.9570    0.018285   \n",
       "27303  3.845482e+05    4.0662    317225.0  1.270574e+06   3.9813    0.006030   \n",
       "27304  2.305792e+05    4.0541    342600.0  1.376049e+06   4.0055    0.006067   \n",
       "...             ...       ...         ...           ...      ...         ...   \n",
       "27995  1.036837e+06    6.0933   1165100.0  7.063849e+06   5.8991    0.012316   \n",
       "27996  1.481691e+06    6.2875   1032600.0  6.338871e+06   5.9840    0.009906   \n",
       "27997  1.651576e+06    6.3118   1282200.0  8.072124e+06   6.0933    0.009748   \n",
       "27998  9.926983e+05    6.2147    760400.0  4.684179e+06   5.9840    0.004036   \n",
       "27999  6.271132e+05    6.1297    413800.0  2.523245e+06   6.0204    0.004020   \n",
       "\n",
       "       price_diff_max  \n",
       "27300        0.056608  \n",
       "27301        0.012108  \n",
       "27302        0.018285  \n",
       "27303        0.009143  \n",
       "27304        0.006067  \n",
       "...               ...  \n",
       "27995        0.012316  \n",
       "27996        0.011900  \n",
       "27997        0.015594  \n",
       "27998        0.013704  \n",
       "27999        0.009982  \n",
       "\n",
       "[700 rows x 19 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lst[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19831089406159733\n",
      "0.05364601690332209\n",
      "-0.3062789002636208\n",
      "0.01622632903626396\n",
      "0.09511182075185708\n",
      "-0.01006439966525965\n",
      "-0.10251804290061226\n",
      "0.30591115280576964\n",
      "-0.11579097887144003\n",
      "0.03952728709967024\n",
      "0.028811495398674652\n",
      "0.06057172604467691\n",
      "-0.28159705305171234\n",
      "0.10268381278415169\n",
      "0.009533480857060404\n",
      "0.05277445513161012\n",
      "-0.2309356590557613\n",
      "-0.020846335328986103\n",
      "-0.02959275982633177\n",
      "-0.0034618287433691346\n",
      "0.13999356562977067\n",
      "-0.0964987746184109\n",
      "0.04372298100219278\n",
      "0.037539493505887325\n",
      "0.009265384512949405\n",
      "-0.15332931368585212\n",
      "0.5200897549488844\n",
      "-0.16254299003439124\n",
      "0.03373950039748678\n",
      "0.1536748471340875\n",
      "0.15163782071014814\n",
      "0.08983602520484181\n",
      "-0.06866561834081718\n",
      "0.12284477175489802\n",
      "-0.09892746104012566\n",
      "-0.15830295476826847\n",
      "-0.1687512567715268\n",
      "-0.1468876486594307\n",
      "-0.17390732728452418\n",
      "-0.17956070726544088\n",
      "0.09021835235447259\n",
      "-0.048720628353511486\n",
      "-0.07197910819153865\n",
      "0.08356343695960823\n",
      "-0.08190306685027408\n",
      "-0.04064739021762619\n",
      "-0.1660544403075676\n",
      "-0.06071787164834663\n",
      "-0.05914086857211519\n",
      "-0.08588309330189542\n",
      "-0.0222428963048301\n",
      "0.010863346979084802\n",
      "0.09878504975564067\n",
      "0.061398825194564174\n"
     ]
    }
   ],
   "source": [
    "for i in range(54):\n",
    "    model = make_pipeline(StandardScaler(),SGDRegressor(max_iter=10000, tol=1e-6,penalty='l1',random_state=1))\n",
    "    pred_temp,score = evaluate2(model, train_lst[i], test_lst[i], train_y_lst[i], real_y_lst[i])\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is the 0 -th model now\n",
      "it is the 1 -th model now\n",
      "it is the 2 -th model now\n",
      "it is the 3 -th model now\n",
      "it is the 4 -th model now\n",
      "it is the 5 -th model now\n",
      "it is the 6 -th model now\n",
      "it is the 7 -th model now\n",
      "it is the 8 -th model now\n",
      "it is the 9 -th model now\n",
      "it is the 10 -th model now\n",
      "it is the 11 -th model now\n",
      "it is the 12 -th model now\n",
      "it is the 13 -th model now\n",
      "it is the 14 -th model now\n",
      "it is the 15 -th model now\n",
      "it is the 16 -th model now\n",
      "it is the 17 -th model now\n",
      "it is the 18 -th model now\n",
      "it is the 19 -th model now\n",
      "it is the 20 -th model now\n",
      "it is the 21 -th model now\n",
      "it is the 22 -th model now\n",
      "it is the 23 -th model now\n",
      "it is the 24 -th model now\n",
      "it is the 25 -th model now\n",
      "it is the 26 -th model now\n",
      "it is the 27 -th model now\n",
      "it is the 28 -th model now\n",
      "it is the 29 -th model now\n",
      "it is the 30 -th model now\n",
      "it is the 31 -th model now\n",
      "it is the 32 -th model now\n",
      "it is the 33 -th model now\n",
      "it is the 34 -th model now\n",
      "it is the 35 -th model now\n",
      "it is the 36 -th model now\n",
      "it is the 37 -th model now\n",
      "it is the 38 -th model now\n",
      "it is the 39 -th model now\n",
      "it is the 40 -th model now\n",
      "it is the 41 -th model now\n",
      "it is the 42 -th model now\n",
      "it is the 43 -th model now\n",
      "it is the 44 -th model now\n",
      "it is the 45 -th model now\n",
      "it is the 46 -th model now\n",
      "it is the 47 -th model now\n",
      "it is the 48 -th model now\n",
      "it is the 49 -th model now\n",
      "it is the 50 -th model now\n",
      "it is the 51 -th model now\n",
      "it is the 52 -th model now\n",
      "it is the 53 -th model now\n",
      "     index         0\n",
      "0        0  0.005302\n",
      "1        1  0.003639\n",
      "2        2  0.002834\n",
      "3        3  0.005365\n",
      "4        4  0.007082\n",
      "..     ...       ...\n",
      "695    695 -0.025808\n",
      "696    696 -0.026869\n",
      "697    697 -0.040641\n",
      "698    698 -0.031296\n",
      "699    699 -0.025180\n",
      "\n",
      "[700 rows x 2 columns]\n",
      "correlation is: -0.05586036043463411\n",
      "-0.05586036043463411 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# model = LinearRegression()\n",
    "model_lst = []\n",
    "for i in range(100):\n",
    "    model_lst.append(make_pipeline(StandardScaler(),SGDRegressor(max_iter=10000, tol=1e-6,penalty='l1',random_state=i)))\n",
    "    # model_lst.append(linear_model.Lasso(alpha=i))\n",
    "    # model_lst.append(LinearRegression())\n",
    "for i in range(54):\n",
    "    pred_lst = []\n",
    "    score_lst = []\n",
    "    temp_score_lst = []\n",
    "    idx_lst = []\n",
    "    for j in range(100):\n",
    "        model = model_lst[j]\n",
    "        pred_temp, score = evaluate2(model, train_lst[i], test_lst[i], train_y_lst[i], real_y_lst[i])\n",
    "        temp_score_lst.append(score)\n",
    "        # print(temp_score_lst)\n",
    "    max_idx = temp_score_lst.index(max(temp_score_lst))\n",
    "    # print(\"maxindex\",max_idx)\n",
    "    pred_store,score = evaluate2(model_lst[max_idx], train_lst[i], test_lst[i], train_y_lst[i], real_y_lst[i])\n",
    "    # print('highest score',score)\n",
    "    pred_lst.append(pred_store)\n",
    "    print(\"it is the\",i,'-th model now')\n",
    "pred_com = pd.concat(pred_lst,axis = 0)\n",
    "pred_com = pred_com.reset_index()\n",
    "print(pred_com)\n",
    "combined = pd.concat([pred_com[:37692],real_return],axis = 1)\n",
    "print(\"correlation is:\", combined.corr().iloc[1,2])\n",
    "score_lst.append(combined.corr().iloc[1,2])\n",
    "\n",
    "print(max(score_lst), score_lst.index(max(score_lst)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/joellau/Desktop/qids/py/model.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pred \u001b[39m=\u001b[39m evaluate4(model, train, test, train_y, real_return)\n",
      "\u001b[1;32m/Users/joellau/Desktop/qids/py/model.ipynb Cell 18\u001b[0m in \u001b[0;36mevaluate4\u001b[0;34m(model, train_lst, test_lst, train_y_lst, real_y_lst)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X22sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m54\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     model_sample \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(model)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X22sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     model_sample\u001b[39m.\u001b[39mfit(train_lst[i], train_y_lst[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X22sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     fit_lst\u001b[39m.\u001b[39mappend(model_sample\u001b[39m.\u001b[39mpredict(train_lst[i])) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     pred_lst\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39marray(model_sample\u001b[39m.\u001b[39mpredict(test_lst[i]))\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "pred = evaluate4(model, train, test, train_y, real_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37795</th>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37796</th>\n",
       "      <td>-0.004109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37797</th>\n",
       "      <td>-0.005192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37798</th>\n",
       "      <td>-0.002927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37799</th>\n",
       "      <td>-0.001874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      0.000878\n",
       "1      0.001435\n",
       "2      0.001100\n",
       "3      0.000901\n",
       "4      0.000411\n",
       "...         ...\n",
       "37795 -0.001919\n",
       "37796 -0.004109\n",
       "37797 -0.005192\n",
       "37798 -0.002927\n",
       "37799 -0.001874\n",
       "\n",
       "[37800 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 10\n",
      "          return         0\n",
      "return  1.000000  0.074379\n",
      "0       0.074379  1.000000\n",
      "          0         0\n",
      "0  1.000000  0.062467\n",
      "0  0.062467  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "for i in [10]:\n",
    "    reg = Ridge(alpha=i)\n",
    "    print(\"i =\",i)\n",
    "    pred = evaluate2(reg, train, test, train_y, real_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.44740422e-06, -4.37143067e-07,  5.89291065e-09,  4.55263535e-11,\n",
       "       -3.04095352e-10,  2.56186426e-11])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 7\n",
      "          return         0\n",
      "return  1.000000  0.069437\n",
      "0       0.069437  1.000000\n",
      "          0         0\n",
      "0  1.000000  0.059406\n",
      "0  0.059406  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "for i in [7]:\n",
    "    print(\"i =\",i)\n",
    "    reg = linear_model.Lasso(alpha=i)\n",
    "    pred = evaluate2(reg, train, test, train_y, real_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00000000e+00, -2.27469085e-07, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.99777096e-10,\n",
       "        1.75605909e-10,  0.00000000e+00, -1.79070137e-09,  3.12846753e-11,\n",
       "        0.00000000e+00, -0.00000000e+00, -5.36817865e-10,  1.47841542e-08,\n",
       "       -2.15848399e-10, -3.22466059e-16,  1.81756503e-19, -0.00000000e+00,\n",
       "       -0.00000000e+00])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          return         0\n",
      "return  1.000000  0.034036\n",
      "0       0.034036  1.000000\n",
      "          0         0\n",
      "0  1.000000  0.037487\n",
      "0  0.037487  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "regr = make_pipeline(StandardScaler(),LinearSVR(random_state=0, tol=1e-5))\n",
    "pred = evaluate2(regr, train, test, train_y, real_return)\n",
    "# evaluate2(model, train, test, train_y,test_y ,return_pred=True, version=2, return_auc=False, plot_auc=False):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          return         0\n",
      "return  1.000000  0.145667\n",
      "0       0.145667  1.000000\n",
      "          0         0\n",
      "0  1.000000 -0.002612\n",
      "0 -0.002612  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.1))\n",
    "pred = evaluate2(regr, train, test, train_y, real_return)\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the pipeline\n",
    "# regr = make_pipeline(StandardScaler(), SVR())\n",
    "\n",
    "# # Define the hyperparameters to search over\n",
    "# param_grid = {\n",
    "#     'svr__C': [0.1, 1, 10],\n",
    "#     'svr__gamma': [0.1, 1, 10],\n",
    "#     'svr__epsilon': [0.1, 0.01, 0.001],\n",
    "# }\n",
    "\n",
    "# # Perform grid search with cross-validation\n",
    "# grid_search = GridSearchCV(regr, param_grid, cv=5, n_jobs=-1)\n",
    "# grid_search.fit(train, train_y)\n",
    "\n",
    "# # Print the best hyperparameters and test score\n",
    "# print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "# print(\"Test score: \", grid_search.score(test, real_return))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          return         0\n",
      "return  1.000000  0.170887\n",
      "0       0.170887  1.000000\n",
      "          0         0\n",
      "0  1.000000  0.015523\n",
      "0  0.015523  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, ensemble\n",
    "\n",
    "for l in [0.99]:\n",
    "    params = {\n",
    "        \"n_estimators\": 200,\n",
    "        \"max_depth\": 3,\n",
    "        \"min_samples_split\": 8,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"loss\": \"huber\",\n",
    "        \"alpha\": l,\n",
    "    }\n",
    "\n",
    "    reg = ensemble.GradientBoostingRegressor(**params)\n",
    "    pred = evaluate2(reg, train, test, train_y, real_return)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061560105033462806\n",
      "0.061560105033462806\n",
      "0.061560105033462806\n",
      "0.06807174173976324\n",
      "0.06807174173976324\n",
      "0.06807174173976324\n",
      "0.06199096481704358\n",
      "0.06199096481704358\n",
      "0.06199096481704358\n",
      "0.05999080175204827\n",
      "0.05999080175204827\n",
      "0.05999080175204827\n",
      "0.06292973318709269\n",
      "0.06292973318709269\n",
      "0.06292973318709269\n",
      "0.050925137454654934\n",
      "0.050925137454654934\n",
      "0.050925137454654934\n",
      "0.04871031743847376\n",
      "0.04871031743847376\n",
      "0.04871031743847376\n",
      "0.06362077173358995\n",
      "0.06362077173358995\n",
      "0.06362077173358995\n",
      "0.05045546512997368\n",
      "0.05045546512997368\n",
      "0.05045546512997368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joellau/Desktop/qids/py/model.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m reg \u001b[39m=\u001b[39m make_pipeline(StandardScaler(),SGDRegressor(max_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, tol\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m,penalty\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m,random_state\u001b[39m=\u001b[39mi))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# pred = evaluate2(reg, train, test, train_y, real_return)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pred, score \u001b[39m=\u001b[39m evaluate3(reg, train, test, train_y, real_return)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(score)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m score_lst\u001b[39m.\u001b[39mappend(score)\n",
      "\u001b[1;32m/Users/joellau/Desktop/qids/py/model.ipynb Cell 38\u001b[0m in \u001b[0;36mevaluate3\u001b[0;34m(model, train, test, train_y, real_y, return_pred, version, return_auc, plot_auc)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate3\u001b[39m(model, train, test, train_y, real_y, return_pred\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, version\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, return_auc\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, plot_auc\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(train, train_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m version \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joellau/Desktop/qids/py/model.ipynb#X43sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         model_train_y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    393\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 394\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1537\u001b[0m, in \u001b[0;36mBaseSGDRegressor.fit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, coef_init\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, intercept_init\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1513\u001b[0m     \u001b[39m\"\"\"Fit linear model with Stochastic Gradient Descent.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[1;32m   1515\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[39m        Fitted `SGDRegressor` estimator.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1537\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m   1538\u001b[0m         X,\n\u001b[1;32m   1539\u001b[0m         y,\n\u001b[1;32m   1540\u001b[0m         alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[1;32m   1541\u001b[0m         C\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m   1542\u001b[0m         loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m   1543\u001b[0m         learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[1;32m   1544\u001b[0m         coef_init\u001b[39m=\u001b[39;49mcoef_init,\n\u001b[1;32m   1545\u001b[0m         intercept_init\u001b[39m=\u001b[39;49mintercept_init,\n\u001b[1;32m   1546\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1547\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1485\u001b[0m, in \u001b[0;36mBaseSGDRegressor._fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[39m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_ \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[0;32m-> 1485\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[1;32m   1486\u001b[0m     X,\n\u001b[1;32m   1487\u001b[0m     y,\n\u001b[1;32m   1488\u001b[0m     alpha,\n\u001b[1;32m   1489\u001b[0m     C,\n\u001b[1;32m   1490\u001b[0m     loss,\n\u001b[1;32m   1491\u001b[0m     learning_rate,\n\u001b[1;32m   1492\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1493\u001b[0m     sample_weight,\n\u001b[1;32m   1494\u001b[0m     coef_init,\n\u001b[1;32m   1495\u001b[0m     intercept_init,\n\u001b[1;32m   1496\u001b[0m )\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39m>\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[1;32m   1501\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n\u001b[1;32m   1502\u001b[0m ):\n\u001b[1;32m   1503\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1504\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMaximum number of iteration reached before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1505\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconvergence. Consider increasing max_iter to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1506\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimprove the fit.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1507\u001b[0m         ConvergenceWarning,\n\u001b[1;32m   1508\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1415\u001b[0m, in \u001b[0;36mBaseSGDRegressor._partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_average_coef \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_features, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1413\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_average_intercept \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1415\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_regressor(\n\u001b[1;32m   1416\u001b[0m     X, y, alpha, C, loss, learning_rate, sample_weight, max_iter\n\u001b[1;32m   1417\u001b[0m )\n\u001b[1;32m   1419\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1618\u001b[0m, in \u001b[0;36mBaseSGDRegressor._fit_regressor\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     average_coef \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# Not used\u001b[39;00m\n\u001b[1;32m   1616\u001b[0m     average_intercept \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]  \u001b[39m# Not used\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m coef, intercept, average_coef, average_intercept, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _plain_sgd(\n\u001b[1;32m   1619\u001b[0m     coef,\n\u001b[1;32m   1620\u001b[0m     intercept[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1621\u001b[0m     average_coef,\n\u001b[1;32m   1622\u001b[0m     average_intercept[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1623\u001b[0m     loss_function,\n\u001b[1;32m   1624\u001b[0m     penalty_type,\n\u001b[1;32m   1625\u001b[0m     alpha,\n\u001b[1;32m   1626\u001b[0m     C,\n\u001b[1;32m   1627\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[1;32m   1628\u001b[0m     dataset,\n\u001b[1;32m   1629\u001b[0m     validation_mask,\n\u001b[1;32m   1630\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mearly_stopping,\n\u001b[1;32m   1631\u001b[0m     validation_score_cb,\n\u001b[1;32m   1632\u001b[0m     \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter_no_change),\n\u001b[1;32m   1633\u001b[0m     max_iter,\n\u001b[1;32m   1634\u001b[0m     tol,\n\u001b[1;32m   1635\u001b[0m     \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept),\n\u001b[1;32m   1636\u001b[0m     \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose),\n\u001b[1;32m   1637\u001b[0m     \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshuffle),\n\u001b[1;32m   1638\u001b[0m     seed,\n\u001b[1;32m   1639\u001b[0m     \u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m   1640\u001b[0m     \u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m   1641\u001b[0m     learning_rate_type,\n\u001b[1;32m   1642\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meta0,\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_t,\n\u001b[1;32m   1644\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   1645\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_,\n\u001b[1;32m   1646\u001b[0m     intercept_decay,\n\u001b[1;32m   1647\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maverage,\n\u001b[1;32m   1648\u001b[0m )\n\u001b[1;32m   1650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1652\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "score_lst = []\n",
    "# for i in [6508,36400]:\n",
    "for i in range(1000,10000):\n",
    "    for j in [1e-4,1e-5,1e-6]:\n",
    "        reg = make_pipeline(StandardScaler(),SGDRegressor(max_iter=10000, tol=1e-6,penalty='l1',random_state=i))\n",
    "        # pred = evaluate2(reg, train, test, train_y, real_return)\n",
    "        pred, score = evaluate3(reg, train, test, train_y, real_return)\n",
    "        print(score)\n",
    "        score_lst.append(score)\n",
    "print(max(score_lst), score_lst.index(max(score_lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00083867],\n",
       "       [ 0.00175657],\n",
       "       [ 0.00125077],\n",
       "       ...,\n",
       "       [-0.00226737],\n",
       "       [-0.00159325],\n",
       "       [-0.00088492]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          return         0\n",
      "return  1.000000  0.231504\n",
      "0       0.231504  1.000000\n",
      "          0         0\n",
      "0  1.000000  0.032785\n",
      "0  0.032785  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "regr = RandomForestRegressor(max_depth=7, random_state=6508, max_features=2, min_samples_leaf=4, min_samples_split=8, n_estimators=300)\n",
    "pred = evaluate2(regr, train, test, train_y, real_return)\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [7, 8, 9, 10],\n",
    "#     'max_features': [2, 3],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "#     'n_estimators': [100, 200, 300, 1000]\n",
    "# }\n",
    "\n",
    "# # Create a based model\n",
    "# rf = RandomForestRegressor()\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                           cv = 3, n_jobs = -1, verbose = 2)\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(train, train_y)\n",
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.18817306e-03, -5.03549075e-03, -5.15205865e-03, -6.13767062e-03,\n",
       "       -6.17336116e-03, -7.65731977e-03, -7.08189447e-03, -2.95779751e-03,\n",
       "       -4.87115217e-03, -8.03174853e-03, -6.96429590e-03, -8.36665442e-03,\n",
       "       -6.57850873e-03, -7.49549072e-03, -6.15497027e-03, -7.59540658e-03,\n",
       "       -6.30651724e-03, -1.26222106e-03, -7.57796735e-03, -6.55519578e-03,\n",
       "       -8.02599537e-03,  2.52904472e-02,  3.99932029e-02, -4.14283803e-04,\n",
       "       -5.57382300e-03, -6.46932151e-03, -7.43853512e-03, -7.74659080e-03,\n",
       "       -8.14983805e-03, -6.95487074e-03, -4.34695932e-03, -5.42129580e-03,\n",
       "       -5.09428038e-03, -2.47650310e-03, -5.72436558e-03, -6.67787559e-03,\n",
       "       -7.23372951e-03, -8.68383356e-03, -4.66569681e-03, -4.18243040e-03,\n",
       "       -7.68435363e-03, -7.69498664e-03, -2.93341994e-03,  2.38353844e-03,\n",
       "       -3.99585507e-03, -6.09223077e-03, -3.26634000e-03, -4.88251374e-03,\n",
       "       -5.95786986e-03, -5.28317267e-03, -6.16077183e-03, -6.51464957e-03,\n",
       "       -5.03345292e-03, -5.46625022e-03, -5.20853026e-03, -7.04607232e-03,\n",
       "       -7.46931573e-03, -4.72303067e-03, -5.96061551e-03, -6.43268438e-03,\n",
       "       -6.96340778e-03, -6.44461790e-03, -5.07874739e-03, -4.75159636e-03,\n",
       "       -4.86593733e-03, -5.49342982e-03, -3.88566311e-03, -6.16837540e-03,\n",
       "       -6.11446388e-03, -5.91117712e-03, -6.13076342e-03, -5.74439706e-03,\n",
       "       -6.13499697e-03, -5.18190403e-03, -6.29974435e-03, -5.61111306e-03,\n",
       "       -6.13842825e-03, -5.75638790e-03, -4.88374942e-03, -5.29908855e-03,\n",
       "       -5.85450782e-03,  2.28303942e-03, -5.14774640e-03, -6.15050363e-03,\n",
       "       -5.44110938e-03, -6.03735696e-03, -5.15837396e-03, -5.42686903e-03,\n",
       "       -3.66855163e-03, -3.93339306e-03, -5.39777861e-03, -5.73679028e-03,\n",
       "       -6.46217908e-03, -5.48184822e-03, -4.77804668e-03, -4.99813411e-03,\n",
       "       -3.49845953e-03, -2.65201146e-03, -3.11579363e-03, -2.39143823e-03,\n",
       "       -4.03803842e-03, -4.30226575e-03, -3.49800009e-03, -3.72307368e-03,\n",
       "       -3.71003953e-03, -3.15625457e-03, -4.48164005e-03, -3.74406262e-03,\n",
       "       -4.73129040e-03, -3.98670695e-03, -3.85218395e-03, -3.83757506e-03,\n",
       "       -3.41308129e-03, -3.01908803e-03, -3.72853698e-03, -3.84818866e-03,\n",
       "       -3.24258286e-03, -3.41383717e-03, -3.20561787e-03, -3.65142146e-03,\n",
       "       -4.42256362e-03, -3.94008203e-03, -2.59546766e-03, -4.12572783e-03,\n",
       "       -2.51466831e-03, -3.09286034e-03, -3.05653961e-03, -2.60210130e-03,\n",
       "       -2.41563154e-03, -2.87790629e-03, -2.84540084e-03, -3.83470305e-03,\n",
       "       -3.00485760e-03, -3.09050727e-03, -2.54939424e-03, -2.63150749e-03,\n",
       "       -1.82607542e-03, -6.28790754e-03, -1.97374475e-03, -4.49940630e-03,\n",
       "       -4.03717792e-03, -2.69240003e-03, -4.52470772e-03, -3.92521788e-03,\n",
       "       -5.61607193e-03, -4.43366861e-03, -2.49158360e-03, -1.58521284e-03,\n",
       "       -3.68664986e-03, -3.07736688e-03, -4.88937634e-03, -4.61682114e-03,\n",
       "       -4.87445843e-03, -3.22336633e-03, -3.28345347e-03, -4.84923723e-03,\n",
       "       -6.32061031e-03, -5.70685881e-03, -6.26451981e-03, -4.73804428e-03,\n",
       "       -3.45257682e-03, -2.34120132e-03,  1.07995613e-02,  7.72690092e-03,\n",
       "        1.38220744e-03,  4.06778999e-03,  4.52880588e-03,  1.05515017e-04,\n",
       "       -2.97016876e-03, -3.48254539e-03, -1.39557233e-03, -1.54673672e-03,\n",
       "       -2.30171545e-03, -2.79609252e-03, -1.97279167e-03, -2.37732799e-03,\n",
       "       -1.78618479e-03, -2.43922011e-03, -2.76219738e-03, -3.80859809e-03,\n",
       "       -2.50157542e-03, -3.25325659e-03, -3.00935983e-03, -2.91064277e-03,\n",
       "       -1.68330528e-03, -3.12453795e-03, -1.61389195e-03, -2.66661293e-03,\n",
       "       -3.02609447e-03, -2.38159031e-03, -3.58515265e-03, -1.16462185e-03,\n",
       "       -2.52922742e-03, -1.65938250e-03, -3.27226218e-03, -1.45373253e-03,\n",
       "       -2.82283649e-03, -4.88377865e-03, -3.12665889e-03, -3.93903800e-03,\n",
       "       -4.79598919e-03, -3.72122225e-03, -2.18019945e-03, -2.71198295e-03,\n",
       "        2.70738861e-04,  9.98619751e-04, -2.56977759e-03, -3.34664591e-03,\n",
       "       -2.60642876e-03, -2.43136506e-03, -3.78740050e-03, -3.37671069e-03,\n",
       "       -7.56437787e-04, -4.92719053e-03, -7.02197857e-03, -4.03590116e-03,\n",
       "       -4.29677391e-03, -6.50551743e-03, -3.85210673e-03, -5.14287577e-03,\n",
       "       -6.88383728e-03, -3.36949418e-03, -4.10695321e-04, -2.83782684e-03,\n",
       "       -5.73590832e-03, -3.96106654e-03, -2.33917585e-03, -4.06003057e-03,\n",
       "       -2.81390212e-03, -2.91350223e-03, -4.24978837e-03, -2.98124255e-03,\n",
       "       -4.43505845e-03, -3.73188715e-03, -3.83115816e-03, -8.81905379e-03,\n",
       "       -6.14345177e-03, -7.31672212e-03,  3.72372173e-03, -6.79590831e-03,\n",
       "       -9.34892864e-03, -5.71732880e-03, -3.51879851e-03, -5.77640443e-03,\n",
       "       -4.19162875e-03, -4.74136753e-03, -4.74309137e-03, -5.38001130e-03,\n",
       "       -5.68541688e-03, -6.71993319e-03, -3.89386572e-03, -4.44582569e-03,\n",
       "       -5.45477441e-03, -5.10130949e-03, -8.93776939e-03, -2.02437153e-03,\n",
       "        9.81394602e-04, -2.91527475e-03,  7.61633675e-03,  1.08878193e-02,\n",
       "       -9.94566080e-03, -2.81561373e-03,  1.55124969e-02, -2.00211331e-02,\n",
       "       -1.45638374e-02, -1.48745382e-02, -1.15396807e-02, -6.94493792e-03,\n",
       "        3.52124471e-03, -1.51899081e-02, -1.20514400e-02, -1.74517094e-02,\n",
       "       -1.16845176e-02, -1.26561498e-02, -1.15557845e-02, -4.31471319e-03,\n",
       "       -9.33575977e-03, -5.87395069e-03, -7.43137515e-03, -1.08985899e-02,\n",
       "       -9.74529127e-03, -7.24464369e-03, -7.19645975e-03, -8.10708852e-03,\n",
       "       -9.68273246e-03, -9.31110827e-03, -1.18535443e-02, -1.76485159e-02,\n",
       "       -1.36637626e-02, -1.29206709e-02, -9.85304197e-03, -1.63552586e-02,\n",
       "       -1.15540354e-02, -1.51026236e-02, -1.02120597e-02, -1.23657532e-02,\n",
       "       -1.25267137e-02, -9.34023654e-03, -8.78095566e-03, -1.22736136e-02,\n",
       "       -1.04469339e-02, -1.00412102e-02, -1.03317182e-02, -4.35789728e-03,\n",
       "       -9.06275783e-03, -8.06840749e-03, -4.69521158e-04, -9.42542566e-03,\n",
       "       -1.05963151e-02, -7.54261833e-03, -9.01090801e-03, -5.30418335e-03,\n",
       "       -5.51866525e-03, -8.43675102e-03, -5.21692494e-03, -1.06249408e-02,\n",
       "       -1.06832534e-02, -9.54860997e-03, -8.68241747e-03, -9.88570042e-03,\n",
       "       -6.97631721e-03, -4.54938864e-03, -5.84613951e-03, -7.06087652e-03,\n",
       "       -5.57791295e-03, -3.89392862e-03, -6.47483312e-03, -6.07805029e-03,\n",
       "       -7.46860221e-03, -8.44754821e-03, -1.68408333e-03, -6.04021988e-03,\n",
       "       -3.78288728e-03, -7.65014604e-03, -6.45994172e-03, -4.25734478e-03,\n",
       "       -6.18194361e-03, -2.31336813e-03, -2.55517672e-03, -2.38908035e-03,\n",
       "       -4.24877390e-03, -4.92625054e-03, -8.50790549e-03, -3.66581285e-03,\n",
       "       -8.52273027e-03, -1.17033781e-02, -7.66626761e-03, -1.10452015e-02,\n",
       "       -1.21814206e-02, -6.86285113e-03, -1.18992135e-02, -9.77633402e-03,\n",
       "       -1.00079822e-02, -9.08002635e-03, -9.70699483e-03, -1.05504449e-02,\n",
       "       -2.95780528e-03, -6.36935172e-03, -8.66414476e-03, -1.35932140e-02,\n",
       "       -1.31003546e-02, -1.61162605e-02, -1.65734176e-02, -1.18892132e-02,\n",
       "       -8.11069054e-03, -8.63641135e-03, -1.02759709e-02, -4.31948993e-03,\n",
       "       -8.02445384e-03, -2.12107892e-02,  1.82792949e-02, -9.55185935e-03,\n",
       "       -2.34335624e-02, -5.95613039e-03, -1.60841951e-02, -1.27280222e-02,\n",
       "       -1.03350154e-02, -9.56515785e-03, -9.44818496e-03, -1.33624315e-02,\n",
       "       -9.47381740e-03, -1.09917015e-02, -9.59510491e-03, -8.19787921e-03,\n",
       "       -9.82179958e-03, -8.96218823e-03, -6.68148520e-03, -9.35705694e-03,\n",
       "       -7.76137992e-03, -1.01432174e-02, -9.13451591e-03, -6.87433398e-03,\n",
       "       -8.35671397e-03, -6.95150291e-03, -6.08058197e-03, -7.60550944e-03,\n",
       "       -7.79228381e-03, -7.07054456e-03, -6.75545807e-03, -5.06032127e-03,\n",
       "       -7.74860084e-03, -6.80862161e-03, -7.50195030e-03, -6.60309528e-03,\n",
       "       -4.66526427e-03, -6.34494981e-03, -9.98132928e-03, -6.81493149e-04,\n",
       "       -9.20527883e-03, -7.12754076e-03, -7.87270896e-03, -9.25226539e-03,\n",
       "       -6.23014573e-03, -5.29093437e-03, -5.26257776e-03, -7.01298785e-03,\n",
       "       -7.69212212e-03, -6.70870154e-03, -5.14680244e-03, -7.20323349e-03,\n",
       "       -9.05802308e-03, -7.97888084e-03, -8.25561435e-03, -5.54916251e-03,\n",
       "       -5.18094990e-03, -5.50523643e-03, -4.47459174e-03, -2.68506450e-03,\n",
       "       -6.53640174e-03, -5.09013252e-03, -3.65398102e-03, -1.03439940e-02,\n",
       "       -8.09048869e-03, -9.93747334e-03, -1.00928290e-02, -1.28517126e-02,\n",
       "       -1.46842535e-02, -1.28768064e-02, -1.20993425e-02, -1.29037913e-02,\n",
       "       -1.30746908e-02, -1.18288609e-02, -1.60426746e-02, -7.58663822e-03,\n",
       "       -1.12281831e-02, -9.22414173e-03, -1.09592950e-02, -8.90614733e-03,\n",
       "       -9.84299573e-03, -8.21911707e-03, -1.13556951e-02, -6.33386154e-03,\n",
       "       -6.30746615e-03, -7.28996693e-03, -9.43292554e-03, -1.18455998e-02,\n",
       "       -1.03507939e-02, -6.34384160e-03, -1.03255636e-02, -2.10184165e-03,\n",
       "       -1.32944685e-02, -9.30955279e-03, -5.03644915e-03, -5.34145907e-03,\n",
       "       -6.20801676e-03, -8.11641681e-03, -4.36813192e-03, -4.95237055e-03,\n",
       "       -4.17376007e-03, -8.40657847e-04, -4.64380832e-03, -4.14396650e-03,\n",
       "       -5.38997535e-03, -4.65598138e-03, -2.00438118e-03,  2.04792992e-03,\n",
       "       -3.14072101e-03, -4.59056740e-03, -5.35372850e-03, -3.25805372e-03,\n",
       "       -3.51956257e-03, -5.71226725e-03, -7.57742934e-03, -5.75378222e-03,\n",
       "       -6.69418720e-03, -3.60537007e-03, -3.92244463e-03, -5.29042441e-03,\n",
       "       -3.96961997e-03, -4.81170032e-03, -5.88701494e-03, -3.75902193e-03,\n",
       "       -4.16088465e-03, -2.92303629e-08, -4.62388697e-03, -4.62144751e-03,\n",
       "       -5.11011695e-03, -4.69591155e-03, -4.23517408e-03, -2.92472731e-03,\n",
       "       -3.27194641e-03,  2.87282178e-03, -3.23785470e-03, -8.17935908e-03,\n",
       "       -1.17816222e-02, -8.81668622e-03, -7.08478122e-03, -6.14279186e-03,\n",
       "       -5.99759203e-03, -1.06497352e-02, -5.54299465e-03, -8.75113888e-03,\n",
       "       -6.34239814e-03, -7.77088841e-03, -9.91540135e-03, -9.57687728e-03,\n",
       "       -5.00163483e-03, -1.26830435e-02, -1.26290593e-02, -7.00983859e-04,\n",
       "       -9.14450105e-03, -7.37772126e-03, -9.31212317e-03, -8.72857691e-03,\n",
       "       -3.52570738e-03, -6.40919327e-03, -2.54624247e-03, -4.94821049e-03,\n",
       "       -3.31480051e-03, -8.23570987e-03, -6.73755736e-03, -6.36638061e-03,\n",
       "       -5.92752659e-03, -3.29594758e-03, -5.75408542e-03, -5.91954503e-03,\n",
       "       -3.34363664e-03, -5.74259849e-03, -3.89432983e-03, -3.32096227e-03,\n",
       "       -2.28591540e-03, -3.43118384e-03, -1.73636618e-03, -1.70059177e-03,\n",
       "       -2.76504384e-03, -4.21793892e-03, -3.08225080e-03, -2.58812821e-03,\n",
       "       -2.73904248e-03, -1.69021535e-03, -2.08325993e-03, -2.68379200e-03,\n",
       "       -1.73055169e-03, -1.96525185e-03, -4.91372901e-03,  1.10711457e-03,\n",
       "       -2.36911858e-03, -4.10733846e-03, -3.59667254e-03, -1.65075560e-03,\n",
       "       -4.12438709e-03, -2.08476811e-03, -4.68231222e-03, -3.50532432e-03,\n",
       "       -4.39378373e-03, -2.92910593e-03, -1.89514453e-03, -5.87216377e-04,\n",
       "       -2.57489930e-03, -1.82020950e-03, -4.47320272e-04, -2.98026298e-03,\n",
       "       -2.88323415e-03, -2.43399011e-03, -1.37338840e-03, -2.37543281e-03,\n",
       "       -2.60284129e-03, -1.94547192e-03, -4.50148507e-04, -2.32496060e-03,\n",
       "       -2.24997762e-03, -3.43219758e-03, -6.98060774e-04, -1.91069229e-03,\n",
       "       -2.49037460e-03, -1.87679944e-03, -2.26492642e-03, -2.90290843e-03,\n",
       "       -2.87396201e-03, -2.15535520e-03, -3.24058972e-03, -8.74200844e-04,\n",
       "       -1.28597459e-03,  2.29024361e-03, -3.77313535e-03, -2.06412523e-03,\n",
       "       -1.87860045e-03, -2.34542436e-03, -2.02171861e-03, -5.58299945e-03,\n",
       "       -7.26034228e-03, -4.84407818e-03,  3.18317675e-03, -6.96475808e-03,\n",
       "       -4.98455127e-03, -2.51803000e-03,  4.27359285e-03, -9.53391848e-03,\n",
       "       -6.99361377e-03, -3.31217059e-03, -2.73524703e-03, -6.77989196e-03,\n",
       "        1.82055481e-03, -2.36513429e-02, -1.81336988e-02, -6.98028011e-03,\n",
       "       -1.24878052e-02, -1.83577620e-02, -1.52867293e-02, -6.21703387e-03,\n",
       "       -1.87784707e-02, -1.69426510e-02, -1.12098486e-02, -5.74797207e-03,\n",
       "       -1.18141482e-02,  1.85290914e-02, -2.93076027e-02, -1.77579139e-02,\n",
       "        4.97474257e-02, -2.05615979e-02, -4.61326833e-02, -4.11250540e-02,\n",
       "       -4.39398046e-02, -3.22796983e-02, -3.16079672e-02, -2.52233535e-02,\n",
       "       -3.40088735e-02, -3.44914659e-02, -2.49324232e-02, -3.16358184e-02,\n",
       "       -3.72949671e-02, -4.02277518e-02, -5.43831913e-02, -4.01833895e-02,\n",
       "       -2.71480441e-02, -3.31842975e-02, -3.18756288e-02, -4.55542003e-02,\n",
       "       -1.38555231e-02, -4.05591799e-02, -2.80486212e-02, -2.72794831e-02,\n",
       "       -2.60565539e-02, -2.35307737e-02, -1.35154357e-02, -2.92808961e-02,\n",
       "       -2.72503972e-02, -2.64536548e-02, -3.15691112e-02, -3.49063288e-02,\n",
       "       -3.44811427e-02, -3.29565690e-02, -2.84468423e-02, -3.83460390e-02,\n",
       "       -3.47691587e-02, -2.68997302e-02, -2.33034346e-02, -2.77675162e-02,\n",
       "       -2.18508889e-02, -2.25338632e-02, -2.57664747e-02, -2.12363449e-02,\n",
       "       -2.69657128e-02, -1.70928961e-02, -4.24214699e-02, -4.00972395e-02,\n",
       "       -3.45669692e-02, -4.43620281e-02, -4.31523910e-02, -6.04428248e-02,\n",
       "       -9.37462668e-02, -2.20852397e-02, -7.27340932e-02, -7.93761847e-02,\n",
       "       -8.00623619e-02, -7.18159113e-02, -5.99220072e-02, -4.78649618e-02,\n",
       "       -6.11778649e-02, -5.85167142e-02, -7.07044599e-02, -7.00926809e-02,\n",
       "       -4.75812330e-02, -7.47508815e-02, -6.97655622e-02, -7.76469824e-02,\n",
       "       -8.57435284e-02, -1.01676846e-01, -7.35141232e-02, -6.13552719e-02])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_arr = np.array(pred_com.iloc[:,1])\n",
    "submit_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(submit_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "800b2c1448e2ea079d66e8039536a26ab9dab7a446f882031feb022006dbbbf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
